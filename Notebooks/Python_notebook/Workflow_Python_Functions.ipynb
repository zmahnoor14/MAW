{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611cc70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import wget\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f7fb541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pubchempy as pcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42327aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rdkit:Enabling RDKit 2021.09.4 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "from pybatchclassyfire import *\n",
    "from pandas import json_normalize\n",
    "from rdkit import Chem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdFMCS\n",
    "from rdkit.Chem import PandasTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a482c0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNaN(string):\n",
    "    return string != string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd169d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slist_sirius(input_dir, slist_csv, substring=None):\n",
    "\n",
    "    \"\"\"slist_sirius is used to create a tsv file that contains a list of\n",
    "    SMILES. The function also runs the sirius command custom db to create\n",
    "    fingerprints for each SMILES in a folder that we by default name as\n",
    "    SL_Frag/. This fingerprints folder is later used by SIRIUS to use\n",
    "    these compounds as a another small list of compounds to match against\n",
    "    the input spectra fingerprints.\n",
    "    Since SIRIUS doesn't take disconnected structure, Multiply charged,\n",
    "    Incorrect syntax, wild card(*) in smiles; this function removes all\n",
    "    such SMILES from the Suspect List.\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML\n",
    "    files and their respective result directories are stored. For this\n",
    "    function this directory must contain a csv file that has a column\n",
    "    named \"SMILES\".\n",
    "\n",
    "    slist_csv (str): This is the csv file that contains a column of\n",
    "    \"SMILES\". Additionally this file can contain other information\n",
    "    about the compounds, but for this function, column of \"SMILES\",\n",
    "    named as \"SMILES\" is necessary.\n",
    "\n",
    "    substring (list): provide a list of strings of SMILES that\n",
    "    shouldn't be considered, provide a list even if there is one string\n",
    "    that shouldnt be considered. e.g: \"[Fe+2]\".\n",
    "\n",
    "    Returns:\n",
    "    tsv: a tsv file of list of SMILES, named as SL_Sirius.tsv, is stored\n",
    "    in input_dir\n",
    "    directory: a directory with compound fragmentations will be created\n",
    "    in a folder named SL_Frag/ within the same input_dir\n",
    "\n",
    "\n",
    "    Usage:\n",
    "    slist_sirius(\"/user/project/\", \"suspectlist.csv\",\n",
    "    substring = None)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sl = pd.read_csv(slist_csv)\n",
    "\n",
    "    # define function to neutralize the charged SMILES\n",
    "    def neutralize_atoms(mol):\n",
    "\n",
    "        pattern = Chem.MolFromSmarts(\n",
    "            \"[+1!h0!$([*]~[-1,-2,-3,-4]),-1!$([*]~[+1,+2,+3,+4])]\"\n",
    "        )\n",
    "        at_matches = mol.GetSubstructMatches(pattern)\n",
    "        at_matches_list = [y[0] for y in at_matches]\n",
    "        if len(at_matches_list) > 0:\n",
    "            for at_idx in at_matches_list:\n",
    "                atom = mol.GetAtomWithIdx(at_idx)\n",
    "                chg = atom.GetFormalCharge()\n",
    "                hcount = atom.GetTotalNumHs()\n",
    "                atom.SetFormalCharge(0)\n",
    "                atom.SetNumExplicitHs(hcount - chg)\n",
    "                atom.UpdatePropertyCache()\n",
    "        return mol\n",
    "\n",
    "    for i, row in sl.iterrows():\n",
    "        # remove SMILES with wild card\n",
    "        if \"*\" in sl[\"SMILES\"][i]:\n",
    "            sl = sl.drop(labels=i, axis=0)\n",
    "    for i, row in sl.iterrows():\n",
    "        # remove SMILES with any string present in the substring\n",
    "        if substring:\n",
    "            if bool([ele for ele in substring if (ele in sl[\"SMILES\"][i])]):\n",
    "                sl = sl.drop(labels=i, axis=0)\n",
    "    for i, row in sl.iterrows():\n",
    "        if \".\" in sl[\"SMILES\"][i]:\n",
    "            sl.loc[i, \"SMILES\"] = sl[\"SMILES\"][i].split(\".\")[0]\n",
    "    # Neutralize the charged SMILES\n",
    "    for i, row in sl.iterrows():\n",
    "        if \"+\" in sl[\"SMILES\"][i] or \"-\" in sl[\"SMILES\"][i]:\n",
    "            mol = Chem.MolFromSmiles(sl[\"SMILES\"][i])\n",
    "            neutralize_atoms(mol)\n",
    "            sl.loc[i, \"SMILES\"] = Chem.MolToSmiles(mol)\n",
    "\n",
    "            # Remove multiple charged SMILES\n",
    "            if \"+\" in sl[\"SMILES\"][i] or \"-\" in sl[\"SMILES\"][i]:\n",
    "                pos = sl[\"SMILES\"][i].count(\"+\")\n",
    "                neg = sl[\"SMILES\"][i].count(\"-\")\n",
    "                charge = pos + neg\n",
    "                if charge > 1:\n",
    "                    sl = sl.drop(labels=i, axis=0)\n",
    "\n",
    "    slsirius = pd.DataFrame({\"smiles\": sl[\"SMILES\"]})\n",
    "    slsirius.to_csv(input_dir + \"SL_Sirius.tsv\", sep=\"\\t\", header=False, index=False)\n",
    "    os.system(\n",
    "        \"sirius --input \"\n",
    "        + input_dir\n",
    "        + \"SL_Sirius.tsv custom-db --name=SL_Frag --output \"\n",
    "        + input_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e56d0de3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e84f7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_postproc(input_dir, Source=\"all\"):\n",
    "\n",
    "    \"\"\"spec_postproc function processes the resulst from dereplication\n",
    "    using different spectral DBs.\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML\n",
    "    files and their respective result directories are stored.\n",
    "\n",
    "    Source (str): either \"mbank\" or \"hmdb\" or \"gnps\", or \"all\"\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    dataframe: of the paths of the processed DB results\n",
    "\n",
    "\n",
    "    Usage:\n",
    "    spec_postproc(input_dir = \"/user/project/\", Source = \"all\")\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Define scoring for all DBs\n",
    "    def HMDB_Scoring(db, i):\n",
    "        if (\n",
    "            db[\"HMDBintScore\"][i] >= 0.50\n",
    "            and db[\"HMDBmzScore\"][i] >= 0.50\n",
    "            and db[\"HQMatchingPeaks\"][i] / db[\"hQueryTotalPeaks\"][i] >= 0.50\n",
    "        ):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def GNPS_Scoring(db, i):\n",
    "        if (\n",
    "            db[\"GNPSintScore\"][i] >= 0.50\n",
    "            and db[\"GNPSmzScore\"][i] >= 0.50\n",
    "            and db[\"GQMatchingPeaks\"][i] / db[\"gQueryTotalPeaks\"][i] >= 0.50\n",
    "        ):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def MB_Scoring(db, i):\n",
    "        if (\n",
    "            db[\"MBintScore\"][i] >= 0.50\n",
    "            and db[\"MBmzScore\"][i] >= 0.50\n",
    "            and db[\"MQMatchingPeaks\"][i] / db[\"mQueryTotalPeaks\"][i] >= 0.50\n",
    "        ):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # list all files and directories\n",
    "    for entry in os.listdir(input_dir):\n",
    "\n",
    "        if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "            # print(entry)\n",
    "\n",
    "            msp_file = glob.glob(\n",
    "                input_dir + \"/\" + entry + \"/spectral_dereplication\" + \"/*.csv\"\n",
    "            )\n",
    "            # print(msp_file)\n",
    "\n",
    "            if len(msp_file) > 0:\n",
    "\n",
    "                if os.path.exists(msp_file[0]):\n",
    "\n",
    "                    msp = pd.read_csv(msp_file[0])\n",
    "                    # enter the directory with /spectral_dereplication/ results\n",
    "\n",
    "                    # enter the directory with /spectral_dereplication/ results\n",
    "                    # GNPS Results\n",
    "                    if Source == \"gnps\" or Source == \"all\":\n",
    "                        msp[\"gnps_results_csv\"] = np.nan\n",
    "\n",
    "                        # currently only these subsets are removed from the names from GNPS\n",
    "                        matches = [\n",
    "                            \"M+\",\n",
    "                            \"[M\",\n",
    "                            \"M-\",\n",
    "                            \"2M\",\n",
    "                            \"M*\" \"20.0\",\n",
    "                            \"50.0\",\n",
    "                            \"30.0\",\n",
    "                            \"40.0\",\n",
    "                            \"60.0\",\n",
    "                            \"70.0\",\n",
    "                            \"eV\",\n",
    "                            \"Massbank\",\n",
    "                            \"Spectral\",\n",
    "                            \"Match\",\n",
    "                            \"to\",\n",
    "                            \"from\",\n",
    "                            \"NIST14\",\n",
    "                            \"MoNA\",\n",
    "                            \"[IIN-based:\",\n",
    "                            \"[IIN-based\",\n",
    "                            \"on:\",\n",
    "                            \"CCMSLIB00003136269]\",\n",
    "                        ]\n",
    "\n",
    "                        # open another csv path holding empty list, which will be filled\n",
    "                        # with post processed csv results\n",
    "                        # GNPScsvfiles2 = []\n",
    "\n",
    "                        # print(entry)\n",
    "                        # enter the directory with /spectral_dereplication/ results\n",
    "                        sub_dir = (\n",
    "                            input_dir + \"/\" + entry + \"/spectral_dereplication/GNPS/\"\n",
    "                        )\n",
    "\n",
    "                        if os.path.exists(sub_dir):\n",
    "                            files = glob.glob(sub_dir + \"/*.csv\")\n",
    "                            # print(files)\n",
    "\n",
    "                            for mz, row in msp.iterrows():\n",
    "                                # print(msp[\"id_X\"][mz])\n",
    "\n",
    "                                for fls_g in files:\n",
    "\n",
    "                                    if msp[\"id_X\"][mz] in fls_g:\n",
    "\n",
    "                                        gnps_df = pd.read_csv(fls_g)\n",
    "                                        gnps_df = gnps_df.drop_duplicates(\n",
    "                                            subset=[\"GNPSSMILES\"]\n",
    "                                        )\n",
    "\n",
    "                                        if len(gnps_df) > 0:\n",
    "\n",
    "                                            for i, row in gnps_df.iterrows():\n",
    "                                                # if compound name is present\n",
    "\n",
    "                                                if GNPS_Scoring(gnps_df, i):\n",
    "\n",
    "                                                    if not isNaN(\n",
    "                                                        gnps_df[\"GNPScompound_name\"][i]\n",
    "                                                    ):\n",
    "                                                        # split if there is a gap in the names\n",
    "\n",
    "                                                        string_chng = gnps_df[\n",
    "                                                            \"GNPScompound_name\"\n",
    "                                                        ][i].split(\" \")\n",
    "\n",
    "                                                        # create an empty list\n",
    "                                                        newstr = []\n",
    "\n",
    "                                                        # for each part of the string in the names\n",
    "                                                        chng = []\n",
    "\n",
    "                                                        for j in range(\n",
    "                                                            len(string_chng)\n",
    "                                                        ):\n",
    "                                                            # check if the substrings are present in the matches and no - is present\n",
    "\n",
    "                                                            if not any(\n",
    "                                                                x in string_chng[j]\n",
    "                                                                for x in matches\n",
    "                                                            ):  # and not '-' == string_chng[j]:\n",
    "\n",
    "                                                                # IF | and ! not in the substring\n",
    "                                                                if (\n",
    "                                                                    \"|\"\n",
    "                                                                    not in string_chng[\n",
    "                                                                        j\n",
    "                                                                    ]\n",
    "                                                                    or \"!\"\n",
    "                                                                    not in string_chng[\n",
    "                                                                        j\n",
    "                                                                    ]\n",
    "                                                                ):\n",
    "\n",
    "                                                                    newstr.append(\n",
    "                                                                        string_chng[j]\n",
    "                                                                    )\n",
    "                                                                # if | present in the substring\n",
    "                                                                elif (\n",
    "                                                                    \"|\"\n",
    "                                                                    in string_chng[j]\n",
    "                                                                ):\n",
    "\n",
    "                                                                    # split the string\n",
    "                                                                    jlen = string_chng[\n",
    "                                                                        j\n",
    "                                                                    ].split(\"|\")\n",
    "                                                                    # how many substrings are left now\n",
    "                                                                    lst = len(jlen) - 1\n",
    "                                                                    # append this to chng\n",
    "                                                                    chng.append(\n",
    "                                                                        jlen[lst]\n",
    "                                                                    )\n",
    "                                                                    break\n",
    "\n",
    "                                                                    # now append chng to newstr\n",
    "                                                        chng.append(\" \".join(newstr))\n",
    "\n",
    "                                                        # save this as the correct name\n",
    "                                                        gnps_df.loc[\n",
    "                                                            i, \"corr_names\"\n",
    "                                                        ] = chng[0]\n",
    "\n",
    "                                                        if not isNaN(\n",
    "                                                            gnps_df[\"GNPSSMILES\"][i]\n",
    "                                                        ):\n",
    "                                                            if chng == \"\":\n",
    "                                                                break\n",
    "                                                            elif gnps_df[\"GNPSSMILES\"][\n",
    "                                                                i\n",
    "                                                            ].isalpha():\n",
    "                                                                s = pcp.get_compounds(\n",
    "                                                                    chng[0], \"name\"\n",
    "                                                                )\n",
    "                                                                if s:\n",
    "                                                                    for comp in s:\n",
    "                                                                        gnps_df[\n",
    "                                                                            \"GNPSSMILES\"\n",
    "                                                                        ][\n",
    "                                                                            i\n",
    "                                                                        ] = (\n",
    "                                                                            comp.isomeric_smiles\n",
    "                                                                        )\n",
    "                                                                else:\n",
    "                                                                    gnps_df[\n",
    "                                                                        \"GNPSSMILES\"\n",
    "                                                                    ][i] = \"\"\n",
    "                                                    else:\n",
    "                                                        gnps_df[\"GNPSSMILES\"][i] = \"\"\n",
    "                                                else:\n",
    "                                                    gnps_df.drop(\n",
    "                                                        [i], axis=0, inplace=True\n",
    "                                                    )\n",
    "\n",
    "                                            for k, row in gnps_df.iterrows():\n",
    "\n",
    "                                                if isNaN(gnps_df[\"GNPSSMILES\"][k]):\n",
    "\n",
    "                                                    if (\n",
    "                                                        \"[\"\n",
    "                                                        in gnps_df[\"GNPScompound_name\"][\n",
    "                                                            k\n",
    "                                                        ].split(\" \")[-1]\n",
    "                                                    ):\n",
    "                                                        string_chng = gnps_df[\n",
    "                                                            \"GNPScompound_name\"\n",
    "                                                        ][k].split(\"[\")\n",
    "                                                        # print(gnps_df['GNPScompound_name'][i])\n",
    "\n",
    "                                                        # keep_names = []\n",
    "                                                        for j in range(\n",
    "                                                            len(string_chng) - 1\n",
    "                                                        ):\n",
    "                                                            gnps_df.loc[\n",
    "                                                                k, \"corr_names\"\n",
    "                                                            ] == string_chng[j]\n",
    "                                                            s = pcp.get_compounds(\n",
    "                                                                string_chng[j], \"name\"\n",
    "                                                            )\n",
    "\n",
    "                                                            if s:\n",
    "                                                                for comp in s:\n",
    "                                                                    gnps_df[\n",
    "                                                                        \"GNPSSMILES\"\n",
    "                                                                    ][\n",
    "                                                                        k\n",
    "                                                                    ] = (\n",
    "                                                                        comp.isomeric_smiles\n",
    "                                                                    )\n",
    "                                                                    gnps_df.loc[\n",
    "                                                                        k, \"GNPSformula\"\n",
    "                                                                    ] = (\n",
    "                                                                        comp.molecular_formula\n",
    "                                                                    )\n",
    "                                                                    gnps_df.loc[\n",
    "                                                                        k, \"GNPSinchi\"\n",
    "                                                                    ] = Chem.MolToInchi(\n",
    "                                                                        Chem.MolFromSmiles(\n",
    "                                                                            comp.isomeric_smiles\n",
    "                                                                        )\n",
    "                                                                    )\n",
    "\n",
    "                                                            else:\n",
    "                                                                gnps_df[\"GNPSSMILES\"][\n",
    "                                                                    k\n",
    "                                                                ] = \"\"\n",
    "                                                                gnps_df.loc[\n",
    "                                                                    k, \"GNPSformula\"\n",
    "                                                                ] = \"\"\n",
    "                                                                gnps_df.loc[\n",
    "                                                                    k, \"GNPSinchi\"\n",
    "                                                                ] = \"\"\n",
    "                                                if not isNaN(gnps_df[\"GNPSSMILES\"][k]):\n",
    "                                                    try:\n",
    "                                                        sx = pcp.get_compounds(\n",
    "                                                            gnps_df[\"GNPSSMILES\"][k],\n",
    "                                                            \"smiles\",\n",
    "                                                        )\n",
    "                                                        gnps_df.loc[\n",
    "                                                            k, \"GNPSinchi\"\n",
    "                                                        ] = Chem.MolToInchi(\n",
    "                                                            Chem.MolFromSmiles(\n",
    "                                                                comp.isomeric_smiles\n",
    "                                                            )\n",
    "                                                        )\n",
    "                                                        if sx:\n",
    "                                                            sx = str(sx)\n",
    "                                                            comp = pcp.Compound.from_cid(\n",
    "                                                                [\n",
    "                                                                    int(x)\n",
    "                                                                    for x in re.findall(\n",
    "                                                                        r\"\\b\\d+\\b\", sx\n",
    "                                                                    )\n",
    "                                                                ]\n",
    "                                                            )\n",
    "                                                            gnps_df.loc[\n",
    "                                                                k, \"GNPSformula\"\n",
    "                                                            ] = comp.molecular_formula\n",
    "\n",
    "                                                    except Exception:\n",
    "                                                        gnps_df.loc[\n",
    "                                                            k, \"GNPSformula\"\n",
    "                                                        ] = \"\"\n",
    "                                                        gnps_df.loc[k, \"GNPSinchi\"] = \"\"\n",
    "\n",
    "                                        gnps_df = gnps_df.dropna(axis=0, how=\"all\")\n",
    "                                        csvname = (\n",
    "                                            (os.path.splitext(fls_g)[0])\n",
    "                                            + \"proc\"\n",
    "                                            + \".csv\"\n",
    "                                        )\n",
    "                                        gnps_results_csv = csvname.replace(\n",
    "                                            input_dir, \".\"\n",
    "                                        )\n",
    "                                        msp.loc[\n",
    "                                            mz, \"gnps_results_csv\"\n",
    "                                        ] = gnps_results_csv\n",
    "                                        gnps_df.to_csv(csvname)\n",
    "                                        # GNPScsvfiles2.append(csvname)\n",
    "                                    # dict1 = {'GNPSr': GNPScsvfiles2}\n",
    "                                    # df = pd.DataFrame(dict1)\n",
    "                                    # return(df)\n",
    "\n",
    "                    msp.to_csv(msp_file[0])\n",
    "\n",
    "                    # HMDB Results\n",
    "                    if Source == \"hmdb\" or Source == \"all\":\n",
    "\n",
    "                        if not os.path.exists(input_dir + \"/hmdb_dframe_str.csv\"):\n",
    "\n",
    "                            # download SDF structures\n",
    "                            os.system(\n",
    "                                \"wget -P \"\n",
    "                                + input_dir\n",
    "                                + \" https://hmdb.ca/system/downloads/current/structures.zip\"\n",
    "                            )\n",
    "                            os.system(\n",
    "                                \"unzip \"\n",
    "                                + input_dir\n",
    "                                + \"/structures.zip\"\n",
    "                                + \" -d \"\n",
    "                                + input_dir\n",
    "                            )\n",
    "\n",
    "                            # Load the sdf\n",
    "                            dframe = PandasTools.LoadSDF(\n",
    "                                (input_dir + \"/structures.sdf\"),\n",
    "                                idName=\"HMDB_ID\",\n",
    "                                smilesName=\"SMILES\",\n",
    "                                molColName=\"Molecule\",\n",
    "                                includeFingerprints=False,\n",
    "                            )\n",
    "\n",
    "                            dframe = dframe[\n",
    "                                [\n",
    "                                    \"DATABASE_ID\",\n",
    "                                    \"SMILES\",\n",
    "                                    \"INCHI_IDENTIFIER\",\n",
    "                                    \"INCHI_KEY\",\n",
    "                                    \"FORMULA\",\n",
    "                                    \"MOLECULAR_WEIGHT\",\n",
    "                                    \"EXACT_MASS\",\n",
    "                                    \"GENERIC_NAME\",\n",
    "                                    \"SYNONYMS\",\n",
    "                                ]\n",
    "                            ]\n",
    "\n",
    "                        elif os.path.exists(input_dir + \"/hmdb_dframe_str.csv\"):\n",
    "\n",
    "                            dframe = pd.read_csv(\n",
    "                                input_dir + \"/hmdb_dframe_str.csv\", low_memory=False\n",
    "                            )\n",
    "\n",
    "                        # HMDBcsvfiles2 = []\n",
    "                        # print(entry)\n",
    "                        # enter the directory with /spectral_dereplication/ results\n",
    "                        sub_dir = (\n",
    "                            input_dir + \"/\" + entry + \"/spectral_dereplication/HMDB/\"\n",
    "                        )\n",
    "\n",
    "                        if os.path.exists(sub_dir):\n",
    "\n",
    "                            # print(sub_dir)\n",
    "                            files = glob.glob(sub_dir + \"/*.csv\")\n",
    "                            # print(files)\n",
    "                            for mz, row in msp.iterrows():\n",
    "                                # print(msp[\"id_X\"][mz])\n",
    "                                for fls_h in files:\n",
    "                                    if msp[\"id_X\"][mz] in fls_h:\n",
    "                                        hmdb_df = pd.read_csv(fls_h)\n",
    "                                        hmdb_df = hmdb_df.drop_duplicates(\n",
    "                                            subset=[\"HMDBcompoundID\"]\n",
    "                                        )\n",
    "\n",
    "                                        if len(hmdb_df) > 0:\n",
    "                                            print(entry)\n",
    "                                            # merge on basis of id, frame and hmdb result files\n",
    "                                            SmilesHM = pd.merge(\n",
    "                                                hmdb_df,\n",
    "                                                dframe,\n",
    "                                                left_on=hmdb_df.HMDBcompoundID,\n",
    "                                                right_on=dframe.DATABASE_ID,\n",
    "                                            )\n",
    "\n",
    "                                            for i, row in hmdb_df.iterrows():\n",
    "                                                if HMDB_Scoring(hmdb_df, i):\n",
    "\n",
    "                                                    for j, row in SmilesHM.iterrows():\n",
    "\n",
    "                                                        # where index for both match, add the name and SMILES\n",
    "                                                        if (\n",
    "                                                            hmdb_df[\"HMDBcompoundID\"][i]\n",
    "                                                            == SmilesHM[\n",
    "                                                                \"HMDBcompoundID\"\n",
    "                                                            ][j]\n",
    "                                                        ):\n",
    "                                                            hmdb_df.loc[\n",
    "                                                                i, \"HMDBSMILES\"\n",
    "                                                            ] = SmilesHM[\"SMILES\"][\n",
    "                                                                j\n",
    "                                                            ]  # add SMILES\n",
    "                                                            hmdb_df.loc[\n",
    "                                                                i, \"HMDBcompound_name\"\n",
    "                                                            ] = SmilesHM[\n",
    "                                                                \"GENERIC_NAME\"\n",
    "                                                            ][\n",
    "                                                                j\n",
    "                                                            ]  # add name\n",
    "                                                            hmdb_df.loc[\n",
    "                                                                i, \"HMDBformula\"\n",
    "                                                            ] = SmilesHM[\"FORMULA\"][\n",
    "                                                                j\n",
    "                                                            ]  # add formula\n",
    "                                                            # hmdb_df.loc[i, 'HMDBinchi'] = Chem.MolToInchi(Chem.MolFromSmiles(SmilesHM['SMILES'][j]))\n",
    "                                                else:\n",
    "                                                    hmdb_df.drop(\n",
    "                                                        [i], axis=0, inplace=True\n",
    "                                                    )\n",
    "\n",
    "                                        csvname = (\n",
    "                                            (os.path.splitext(fls_h)[0])\n",
    "                                            + \"proc\"\n",
    "                                            + \".csv\"\n",
    "                                        )  # name for writing it in a new file\n",
    "                                        hmdb_results_csv = csvname.replace(\n",
    "                                            input_dir, \".\"\n",
    "                                        )\n",
    "                                        msp.loc[\n",
    "                                            mz, \"hmdb_results_csv\"\n",
    "                                        ] = hmdb_results_csv\n",
    "                                        hmdb_df.to_csv(csvname)  # write\n",
    "                                        # HMDBcsvfiles2.append(csvname)# add to a list\n",
    "                                    # dict1 = {'HMDBr': HMDBcsvfiles2}\n",
    "                                    # df = pd.DataFrame(dict1)\n",
    "                                    # return(df)\n",
    "\n",
    "                    msp.to_csv(msp_file[0])\n",
    "\n",
    "                    # MASSBANK Results\n",
    "\n",
    "                    # enter the directory with /spectral_dereplication/ results\n",
    "                    if Source == \"mbank\" or Source == \"all\":\n",
    "                        # open another csv path holding empty list, which will be filled\n",
    "                        # with post processed csv results\n",
    "                        # MassBankcsvfiles2 = []\n",
    "                        # print(entry)\n",
    "                        # enter the directory with /spectral_dereplication/ results\n",
    "                        sub_dir = (\n",
    "                            input_dir\n",
    "                            + \"/\"\n",
    "                            + entry\n",
    "                            + \"/spectral_dereplication/MassBank/\"\n",
    "                        )\n",
    "                        if os.path.exists(sub_dir):\n",
    "                            files = glob.glob(sub_dir + \"/*.csv\")\n",
    "                            # print(files)\n",
    "                            for mz, row in msp.iterrows():\n",
    "                                # print(msp[\"id_X\"][mz])\n",
    "                                for fls_m in files:\n",
    "                                    if msp[\"id_X\"][mz] in fls_m:\n",
    "                                        print(fls_m)\n",
    "                                        mbank_df = pd.read_csv(fls_m)\n",
    "                                        mbank_df = mbank_df.drop_duplicates(\n",
    "                                            subset=[\"MBSMILES\"]\n",
    "                                        )\n",
    "#                                         if len(mbank_df) > 0:\n",
    "\n",
    "#                                             for i, row in mbank_df.iterrows():\n",
    "#                                                 if MB_Scoring(mbank_df, i):\n",
    "\n",
    "#                                                     inchiK = str(\n",
    "#                                                         mbank_df[\"MBinchiKEY\"][i]\n",
    "#                                                     )\n",
    "\n",
    "#                                                     # extract inchikeys\n",
    "#                                                     y = pcp.get_compounds(\n",
    "#                                                         inchiK, \"inchikey\"\n",
    "#                                                     )  # compound based on inchikey\n",
    "\n",
    "#                                                     for compound in y:\n",
    "\n",
    "#                                                         # add smiles\n",
    "#                                                         smles = compound.isomeric_smiles\n",
    "#                                                         mbank_df.loc[\n",
    "#                                                             i, \"MBSMILES\"\n",
    "#                                                         ] = smles\n",
    "#                                                         # mbank_df.loc[i, 'MBinchi'] =Chem.MolToInchi(Chem.MolFromSmiles(smles))\n",
    "#                                                 else:\n",
    "#                                                     mbank_df.drop(\n",
    "#                                                         [i], axis=0, inplace=True\n",
    "#                                                     )\n",
    "\n",
    "                                        csvname = (\n",
    "                                            (os.path.splitext(fls_m)[0])\n",
    "                                            + \"proc\"\n",
    "                                            + \".csv\"\n",
    "                                        )\n",
    "                                        mbank_results_csv = csvname.replace(\n",
    "                                            input_dir, \".\"\n",
    "                                        )\n",
    "                                        msp.loc[\n",
    "                                            mz, \"mbank_results_csv\"\n",
    "                                        ] = mbank_results_csv\n",
    "                                        mbank_df.to_csv(csvname)\n",
    "                                        # MassBankcsvfiles2.append(csvname)\n",
    "\n",
    "                                    # dict1 = {'MBr': MassBankcsvfiles2}\n",
    "                                    # df = pd.DataFrame(dict1)\n",
    "                                    # return(df)\n",
    "\n",
    "                    msp.to_csv(msp_file[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27899d3a",
   "metadata": {},
   "source": [
    "# SIRIUS Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ef42aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sirius_postproc(input_dir, exp_int=0.90, csi_score=-150):\n",
    "\n",
    "    def str_can_score(db, i):\n",
    "        if (\n",
    "            db[\"explainedIntensity\"][i] >= exp_int\n",
    "            and db[\"CSI:FingerIDScore\"][i] >= csi_score\n",
    "        ):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # entry is all files and folders in input_dir\n",
    "    for entry in os.listdir(input_dir):\n",
    "        # if the entry is also a directory\n",
    "        if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "            sub_dir = input_dir + \"/\" + entry + \"/insilico/SIRIUS/\"\n",
    "            msp_csv = input_dir + \"/\" + entry + \"/insilico/MS1DATA.csv\"\n",
    "            if os.path.exists(msp_csv) and os.path.exists(sub_dir):\n",
    "                # output json files from SIRIUS\n",
    "                files_S = glob.glob(sub_dir + \"/*.json\")\n",
    "                # list of precursor m/z\n",
    "                msp = pd.read_csv(msp_csv)\n",
    "\n",
    "                # for each mz\n",
    "                for mz, row in msp.iterrows():\n",
    "                    # make a list of files with this mz\n",
    "                    files_for_mz = []\n",
    "\n",
    "                    for file in files_S:\n",
    "                        if str(msp[\"premz\"][mz]) in file:\n",
    "                            files_for_mz.append(file)\n",
    "\n",
    "                    # extract the formula and structure files\n",
    "                    json_dirALL = next(os.walk(files_for_mz[0]))[1]\n",
    "                    if len(json_dirALL) > 0:\n",
    "                        sub_sub_dirALL_structure_can = (\n",
    "                            files_for_mz[0]\n",
    "                            + \"/\"\n",
    "                            + json_dirALL[0]\n",
    "                            + \"/structure_candidates.tsv\"\n",
    "                        )\n",
    "                        sub_sub_dirALL_formula_can = (\n",
    "                            files_for_mz[0]\n",
    "                            + \"/\"\n",
    "                            + json_dirALL[0]\n",
    "                            + \"/formula_candidates.tsv\"\n",
    "                        )\n",
    "                        ALL_Canopus_csv = files_for_mz[0] + \"/canopus_summary.tsv\"\n",
    "\n",
    "                        # if both structure files exist\n",
    "                        if (\n",
    "                            os.path.exists(sub_sub_dirALL_structure_can)\n",
    "                            and len(pd.read_csv(sub_sub_dirALL_structure_can, sep=\"\\t\")) > 0\n",
    "                        ):\n",
    "                            if (\n",
    "                                os.path.exists(sub_sub_dirALL_formula_can)\n",
    "                                and len(pd.read_csv(sub_sub_dirALL_formula_can, sep=\"\\t\"))\n",
    "                                > 0\n",
    "                            ):\n",
    "                                ALL_structure_csv = pd.read_csv(\n",
    "                                    sub_sub_dirALL_structure_can, sep=\"\\t\"\n",
    "                                )\n",
    "                                ALL_formula_csv = pd.read_csv(\n",
    "                                    sub_sub_dirALL_formula_can, sep=\"\\t\"\n",
    "                                )\n",
    "                                ALL_Canopus = pd.read_csv(ALL_Canopus_csv, sep=\"\\t\")\n",
    "                                # Add the structure and formula files together\n",
    "                                for structure, rows in ALL_structure_csv.iterrows():\n",
    "                                    for formula, rows in ALL_formula_csv.iterrows():\n",
    "                                        if (\n",
    "                                            ALL_structure_csv[\"formulaRank\"][structure]\n",
    "                                            == ALL_formula_csv[\"rank\"][formula]\n",
    "                                        ):\n",
    "                                            ALL_structure_csv.loc[\n",
    "                                                structure, \"SiriusScore\"\n",
    "                                            ] = ALL_formula_csv[\"SiriusScore\"][formula]\n",
    "                                            ALL_structure_csv.loc[\n",
    "                                                structure, \"numExplainedPeaks\"\n",
    "                                            ] = ALL_formula_csv[\"numExplainedPeaks\"][\n",
    "                                                formula\n",
    "                                            ]\n",
    "                                            ALL_structure_csv.loc[\n",
    "                                                structure, \"explainedIntensity\"\n",
    "                                            ] = ALL_formula_csv[\"explainedIntensity\"][\n",
    "                                                formula\n",
    "                                            ]\n",
    "                                            # ALL_structure_csv.loc[structure, \"SuspectListEntry\"] = \"FALSE\"\n",
    "                                            if len(ALL_Canopus) > 0:\n",
    "                                                if (\n",
    "                                                    ALL_formula_csv[\"molecularFormula\"][\n",
    "                                                        formula\n",
    "                                                    ]\n",
    "                                                    == ALL_Canopus[\"molecularFormula\"][0]\n",
    "                                                ):\n",
    "                                                    ALL_structure_csv.loc[\n",
    "                                                        structure, \"superclass\"\n",
    "                                                    ] = ALL_Canopus[\"superclass\"][0]\n",
    "                                                    ALL_structure_csv.loc[\n",
    "                                                        structure, \"class\"\n",
    "                                                    ] = ALL_Canopus[\"class\"][0]\n",
    "                                                    ALL_structure_csv.loc[\n",
    "                                                        structure, \"subclass\"\n",
    "                                                    ] = ALL_Canopus[\"subclass\"][0]\n",
    "\n",
    "                                for str_siriusA, row in ALL_structure_csv.iterrows():\n",
    "                                    if not str_can_score(ALL_structure_csv, str_siriusA):\n",
    "                                        ALL_structure_csv = ALL_structure_csv.drop(\n",
    "                                            str_siriusA, inplace=False\n",
    "                                        )\n",
    "\n",
    "                            result_sirius_name = (\n",
    "                                sub_dir\n",
    "                                + \"results_for_\"\n",
    "                                + json_dirALL[0].split(\"_\")[-1]\n",
    "                                + \"_\"\n",
    "                                + \"structure.csv\"\n",
    "                            )\n",
    "                            msp.loc[mz, \"sirius_result_dir\"] = result_sirius_name.replace(\n",
    "                                input_dir, \".\"\n",
    "                            )\n",
    "\n",
    "                            ALL_structure_csv.to_csv(result_sirius_name)\n",
    "\n",
    "                        elif not (\n",
    "                            os.path.exists(sub_sub_dirALL_structure_can)\n",
    "                            and len(pd.read_csv(sub_sub_dirALL_structure_can, sep=\"\\t\")) > 0\n",
    "                        ):\n",
    "                            if (\n",
    "                                os.path.exists(sub_sub_dirALL_formula_can)\n",
    "                                and len(pd.read_csv(sub_sub_dirALL_formula_can, sep=\"\\t\"))\n",
    "                                > 0\n",
    "                            ):\n",
    "                                ALL_formula_csv = pd.read_csv(\n",
    "                                    sub_sub_dirALL_formula_can, sep=\"\\t\"\n",
    "                                )\n",
    "                                ALL_Canopus = pd.read_csv(ALL_Canopus_csv, sep=\"\\t\")\n",
    "                                if len(ALL_Canopus) > 0:\n",
    "                                    for formula, rows in ALL_formula_csv.iterrows():\n",
    "                                        ALL_formula_csv.loc[\n",
    "                                            formula, \"superclass\"\n",
    "                                        ] = ALL_Canopus[\"superclass\"][0]\n",
    "                                        ALL_formula_csv.loc[formula, \"class\"] = ALL_Canopus[\n",
    "                                            \"class\"\n",
    "                                        ][0]\n",
    "                                        ALL_formula_csv.loc[\n",
    "                                            formula, \"subclass\"\n",
    "                                        ] = ALL_Canopus[\"subclass\"][0]\n",
    "\n",
    "                                for for_siriusA, row in ALL_formula_csv.iterrows():\n",
    "                                    if (\n",
    "                                        not ALL_formula_csv[\"explainedIntensity\"][\n",
    "                                            for_siriusA\n",
    "                                        ]\n",
    "                                        >= exp_int\n",
    "                                    ):\n",
    "                                        ALL_formula_csv = ALL_formula_csv.drop(\n",
    "                                            for_siriusA, inplace=False\n",
    "                                        )\n",
    "\n",
    "                                result_sirius_name = (\n",
    "                                    sub_dir\n",
    "                                    + \"results_for_\"\n",
    "                                    + json_dirALL[0].split(\"_\")[-1]\n",
    "                                    + \"_\"\n",
    "                                    + \"formula.csv\"\n",
    "                                )\n",
    "                                msp.loc[\n",
    "                                    mz, \"sirius_result_dir\"\n",
    "                                ] = result_sirius_name.replace(input_dir, \".\")\n",
    "\n",
    "                                ALL_formula_csv.to_csv(\n",
    "                                    sub_dir\n",
    "                                    + \"results_for_\"\n",
    "                                    + json_dirALL[0].split(\"_\")[-1]\n",
    "                                    + \"_\"\n",
    "                                    + \"formula.csv\"\n",
    "                                )\n",
    "\n",
    "                            else:\n",
    "                                print(\"no file for formula\")\n",
    "                        else:\n",
    "                            print(\"no file for structure or formula\")\n",
    "            msp.to_csv(msp_csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a3430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def str_can_score(db, i):\n",
    "#     if (\n",
    "#         db[\"explainedIntensity\"][i] >= exp_int\n",
    "#         #and db[\"CSI:FingerIDScore\"][i] >= csi_score\n",
    "#     ):\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False\n",
    "\n",
    "# # entry is all files and folders in input_dir\n",
    "# for entry in os.listdir(input_dir):\n",
    "#     # if the entry is also a directory\n",
    "#     if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "#         sub_dir = input_dir + \"/\" + entry + \"/insilico/SIRIUS/\"\n",
    "#         msp_csv = input_dir + \"/\" + entry + \"/insilico/MS1DATA.csv\"\n",
    "#         if os.path.exists(msp_csv) and os.path.exists(sub_dir):\n",
    "#             # output json files from SIRIUS\n",
    "#             files_S = glob.glob(sub_dir + \"/*.json\")\n",
    "#             # list of precursor m/z\n",
    "#             msp = pd.read_csv(msp_csv)\n",
    "\n",
    "#             # for each mz\n",
    "#             for mz, row in msp.iterrows():\n",
    "#                 # make a list of files with this mz\n",
    "#                 #print(mz)\n",
    "#                 files_for_mz = []\n",
    "\n",
    "#                 for file in files_S:\n",
    "#                     if str(msp[\"premz\"][mz]) in file:\n",
    "#                         files_for_mz.append(file)\n",
    "\n",
    "#                 # extract the formula and structure files\n",
    "#                 json_dirALL = next(os.walk(files_for_mz[0]))[1]\n",
    "#                 if len(json_dirALL)>0:\n",
    "#                     sub_sub_dirALL_structure_can = (\n",
    "#                         files_for_mz[0]\n",
    "#                         + \"/\"\n",
    "#                         + json_dirALL[0]\n",
    "#                         + \"/structure_candidates.tsv\"\n",
    "#                     )\n",
    "#                     sub_sub_dirALL_formula_can = (\n",
    "#                         files_for_mz[0]\n",
    "#                         + \"/\"\n",
    "#                         + json_dirALL[0]\n",
    "#                         + \"/formula_candidates.tsv\"\n",
    "#                     )\n",
    "#                     ALL_Canopus_csv = files_for_mz[0] + \"/canopus_summary.tsv\"\n",
    "\n",
    "#                     # if both structure files exist\n",
    "#                     if (\n",
    "#                         os.path.exists(sub_sub_dirALL_structure_can)\n",
    "#                         and len(pd.read_csv(sub_sub_dirALL_structure_can, sep=\"\\t\")) > 0\n",
    "#                     ):\n",
    "#                         if (\n",
    "#                             os.path.exists(sub_sub_dirALL_formula_can)\n",
    "#                             and len(pd.read_csv(sub_sub_dirALL_formula_can, sep=\"\\t\"))\n",
    "#                             > 0\n",
    "#                         ):\n",
    "#                             ALL_structure_csv = pd.read_csv(\n",
    "#                                 sub_sub_dirALL_structure_can, sep=\"\\t\"\n",
    "#                             )\n",
    "#                             ALL_formula_csv = pd.read_csv(\n",
    "#                                 sub_sub_dirALL_formula_can, sep=\"\\t\"\n",
    "#                             )\n",
    "#                             ALL_Canopus = pd.read_csv(ALL_Canopus_csv, sep=\"\\t\")\n",
    "#                             # Add the structure and formula files together\n",
    "#                             for structure, rows in ALL_structure_csv.iterrows():\n",
    "#                                 for formula, rows in ALL_formula_csv.iterrows():\n",
    "#                                     if (\n",
    "#                                         ALL_structure_csv[\"formulaRank\"][structure]\n",
    "#                                         == ALL_formula_csv[\"rank\"][formula]\n",
    "#                                     ):\n",
    "#                                         ALL_structure_csv.loc[\n",
    "#                                             structure, \"SiriusScore\"\n",
    "#                                         ] = ALL_formula_csv[\"SiriusScore\"][formula]\n",
    "#                                         ALL_structure_csv.loc[\n",
    "#                                             structure, \"numExplainedPeaks\"\n",
    "#                                         ] = ALL_formula_csv[\"numExplainedPeaks\"][\n",
    "#                                             formula\n",
    "#                                         ]\n",
    "#                                         ALL_structure_csv.loc[\n",
    "#                                             structure, \"explainedIntensity\"\n",
    "#                                         ] = ALL_formula_csv[\"explainedIntensity\"][\n",
    "#                                             formula\n",
    "#                                         ]\n",
    "#                                         # ALL_structure_csv.loc[structure, \"SuspectListEntry\"] = \"FALSE\"\n",
    "#                                         if len(ALL_Canopus) > 0:\n",
    "#                                             if (\n",
    "#                                                 ALL_formula_csv[\"molecularFormula\"][\n",
    "#                                                     formula\n",
    "#                                                 ]\n",
    "#                                                 == ALL_Canopus[\"molecularFormula\"][0]\n",
    "#                                             ):\n",
    "#                                                 ALL_structure_csv.loc[\n",
    "#                                                     structure, \"superclass\"\n",
    "#                                                 ] = ALL_Canopus[\"superclass\"][0]\n",
    "#                                                 ALL_structure_csv.loc[\n",
    "#                                                     structure, \"class\"\n",
    "#                                                 ] = ALL_Canopus[\"class\"][0]\n",
    "#                                                 ALL_structure_csv.loc[\n",
    "#                                                     structure, \"subclass\"\n",
    "#                                                 ] = ALL_Canopus[\"subclass\"][0]\n",
    "\n",
    "#                             for str_siriusA, row in ALL_structure_csv.iterrows():\n",
    "#                                 if not str_can_score(ALL_structure_csv, str_siriusA):\n",
    "#                                     ALL_structure_csv = ALL_structure_csv.drop(\n",
    "#                                         str_siriusA, inplace=False\n",
    "#                                     )\n",
    "\n",
    "#                         result_sirius_name = (\n",
    "#                             sub_dir\n",
    "#                             + \"results_for_\"\n",
    "#                             + json_dirALL[0].split(\"_\")[-1]\n",
    "#                             + \"_\"\n",
    "#                             + \"structure.csv\"\n",
    "#                         )\n",
    "#                         msp.loc[mz, \"sirius_result_dir\"] = result_sirius_name.replace(\n",
    "#                             input_dir, \".\"\n",
    "#                         )\n",
    "\n",
    "#                         ALL_structure_csv.to_csv(result_sirius_name)\n",
    "\n",
    "#                     elif not (\n",
    "#                         os.path.exists(sub_sub_dirALL_structure_can)\n",
    "#                         and len(pd.read_csv(sub_sub_dirALL_structure_can, sep=\"\\t\")) > 0\n",
    "#                     ):\n",
    "#                         if (\n",
    "#                             os.path.exists(sub_sub_dirALL_formula_can)\n",
    "#                             and len(pd.read_csv(sub_sub_dirALL_formula_can, sep=\"\\t\"))\n",
    "#                             > 0\n",
    "#                         ):\n",
    "#                             ALL_formula_csv = pd.read_csv(\n",
    "#                                 sub_sub_dirALL_formula_can, sep=\"\\t\"\n",
    "#                             )\n",
    "#                             ALL_Canopus = pd.read_csv(ALL_Canopus_csv, sep=\"\\t\")\n",
    "#                             if len(ALL_Canopus) > 0:\n",
    "#                                 for formula, rows in ALL_formula_csv.iterrows():\n",
    "#                                     ALL_formula_csv.loc[\n",
    "#                                         formula, \"superclass\"\n",
    "#                                     ] = ALL_Canopus[\"superclass\"][0]\n",
    "#                                     ALL_formula_csv.loc[formula, \"class\"] = ALL_Canopus[\n",
    "#                                         \"class\"\n",
    "#                                     ][0]\n",
    "#                                     ALL_formula_csv.loc[\n",
    "#                                         formula, \"subclass\"\n",
    "#                                     ] = ALL_Canopus[\"subclass\"][0]\n",
    "\n",
    "#                             for for_siriusA, row in ALL_formula_csv.iterrows():\n",
    "#                                 if (\n",
    "#                                     not ALL_formula_csv[\"explainedIntensity\"][\n",
    "#                                         for_siriusA\n",
    "#                                     ]\n",
    "#                                     >= exp_int\n",
    "#                                 ):\n",
    "#                                     ALL_formula_csv = ALL_formula_csv.drop(\n",
    "#                                         for_siriusA, inplace=False\n",
    "#                                     )\n",
    "\n",
    "#                             result_sirius_name = (\n",
    "#                                 sub_dir\n",
    "#                                 + \"results_for_\"\n",
    "#                                 + json_dirALL[0].split(\"_\")[-1]\n",
    "#                                 + \"_\"\n",
    "#                                 + \"formula.csv\"\n",
    "#                             )\n",
    "#                             msp.loc[\n",
    "#                                 mz, \"sirius_result_dir\"\n",
    "#                             ] = result_sirius_name.replace(input_dir, \".\")\n",
    "\n",
    "#                             ALL_formula_csv.to_csv(\n",
    "#                                 sub_dir\n",
    "#                                 + \"results_for_\"\n",
    "#                                 + json_dirALL[0].split(\"_\")[-1]\n",
    "#                                 + \"_\"\n",
    "#                                 + \"formula.csv\"\n",
    "#                             )\n",
    "\n",
    "#                         else:\n",
    "#                             print(\"no file for formula\")\n",
    "#                     else:\n",
    "#                         print(\"no file for structure or formula\")\n",
    "#         msp.to_csv(msp_csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "943f9ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCSS_for_SpecDB(input_dir, Source):\n",
    "    # Describe the heavy atoms to be considered for MCSS\n",
    "    heavy_atoms = [\"C\", \"N\", \"P\", \"O\", \"S\"]\n",
    "    # list all files and directories\n",
    "    for entry in os.listdir(input_dir):\n",
    "\n",
    "        if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "\n",
    "            # for specdb\n",
    "            specdb_msp_file = glob.glob(\n",
    "                input_dir + \"/\" + entry + \"/spectral_dereplication\" + \"/*.csv\"\n",
    "            )\n",
    "\n",
    "            if len(specdb_msp_file) > 0:\n",
    "\n",
    "                if os.path.exists(specdb_msp_file[0]):\n",
    "\n",
    "                    spec_msp = pd.read_csv(specdb_msp_file[0])\n",
    "\n",
    "                    for mz, row in spec_msp.iterrows():\n",
    "\n",
    "                        if Source == \"gnps\" or Source == \"specdb\" or Source == \"all\":\n",
    "\n",
    "                            sub_dir = (\n",
    "                                input_dir\n",
    "                                + \"/\"\n",
    "                                + entry\n",
    "                                + \"/spectral_dereplication/GNPS/\"\n",
    "                            )\n",
    "                            if os.path.exists(sub_dir):\n",
    "                                gnps_files = glob.glob(sub_dir + \"/*proc.csv\")\n",
    "\n",
    "                                for files in gnps_files:\n",
    "                                    if spec_msp[\"id_X\"][mz] in files:\n",
    "                                        gnpsproc = pd.read_csv(files)\n",
    "\n",
    "                                        if len(gnpsproc) > 0:\n",
    "                                            G_Smiles = gnpsproc[\"GNPSSMILES\"]\n",
    "                                            G_Smiles = list(filter(None, G_Smiles))\n",
    "                                            # print(G_Smiles)\n",
    "                                            # create empty list of GNPS top smiles\n",
    "                                            GNPS_Mol = []\n",
    "                                            # extract only the InChI of the top 5\n",
    "                                            for j in list(G_Smiles):\n",
    "                                                if not isNaN(j):\n",
    "                                                    print(type(j))\n",
    "                                                    mol2 = Chem.MolFromSmiles(j)\n",
    "                                                    GNPS_Mol.append(mol2)\n",
    "\n",
    "                                            if len(GNPS_Mol) >= 2:\n",
    "                                                res = rdFMCS.FindMCS(GNPS_Mol)\n",
    "                                                sm_res = Chem.MolToSmiles(\n",
    "                                                    Chem.MolFromSmarts(res.smartsString)\n",
    "                                                )\n",
    "                                                # if there are atleast 3 heavy atoms in the MCSS, then add it to the result file\n",
    "                                                elem = [\n",
    "                                                    ele\n",
    "                                                    for ele in heavy_atoms\n",
    "                                                    if (ele in sm_res)\n",
    "                                                ]\n",
    "                                                if elem and len(sm_res) >= 3:\n",
    "                                                    spec_msp.loc[\n",
    "                                                        mz, \"GNPS_MCSSstring\"\n",
    "                                                    ] = res.smartsString\n",
    "                                                    spec_msp.loc[\n",
    "                                                        mz, \"GNPS_MCSS_SMILES\"\n",
    "                                                    ] = Chem.MolToSmiles(\n",
    "                                                        Chem.MolFromSmarts(\n",
    "                                                            res.smartsString\n",
    "                                                        )\n",
    "                                                    )\n",
    "                        if Source == \"hmdb\" or Source == \"specdb\" or Source == \"all\":\n",
    "                            sub_dir = (\n",
    "                                input_dir\n",
    "                                + \"/\"\n",
    "                                + entry\n",
    "                                + \"/spectral_dereplication/HMDB/\"\n",
    "                            )\n",
    "                            if os.path.exists(sub_dir):\n",
    "                                hmdb_files = glob.glob(sub_dir + \"/*proc.csv\")\n",
    "\n",
    "                                for files in hmdb_files:\n",
    "                                    if spec_msp[\"id_X\"][mz] in files:\n",
    "                                        hmdbproc = pd.read_csv(files)\n",
    "\n",
    "                                        if len(hmdbproc) > 0:\n",
    "                                            H_Smiles = hmdbproc[\"HMDBSMILES\"]\n",
    "                                            H_Smiles = list(filter(None, H_Smiles))\n",
    "\n",
    "                                            HMDB_Mol = []\n",
    "                                            # extract only the InChI of the top 5\n",
    "                                            for j in list(H_Smiles):\n",
    "                                                if not isNaN(j):\n",
    "\n",
    "                                                    mol2 = Chem.MolFromSmiles(j)\n",
    "                                                    HMDB_Mol.append(mol2)\n",
    "\n",
    "                                            if len(HMDB_Mol) >= 2:\n",
    "                                                res = rdFMCS.FindMCS(HMDB_Mol)\n",
    "                                                sm_res = Chem.MolToSmiles(\n",
    "                                                    Chem.MolFromSmarts(res.smartsString)\n",
    "                                                )\n",
    "                                                # if there are atleast 3 heavy atoms in the MCSS, then add it to the result file\n",
    "                                                elem = [\n",
    "                                                    ele\n",
    "                                                    for ele in heavy_atoms\n",
    "                                                    if (ele in sm_res)\n",
    "                                                ]\n",
    "                                                if elem and len(sm_res) >= 3:\n",
    "                                                    spec_msp.loc[\n",
    "                                                        mz, \"HMDB_MCSSstring\"\n",
    "                                                    ] = res.smartsString\n",
    "                                                    spec_msp.loc[\n",
    "                                                        mz, \"HMDB_MCSS_SMILES\"\n",
    "                                                    ] = Chem.MolToSmiles(\n",
    "                                                        Chem.MolFromSmarts(\n",
    "                                                            res.smartsString\n",
    "                                                        )\n",
    "                                                    )\n",
    "                        if Source == \"mbank\" or Source == \"specdb\" or Source == \"all\":\n",
    "                            sub_dir = (\n",
    "                                input_dir\n",
    "                                + \"/\"\n",
    "                                + entry\n",
    "                                + \"/spectral_dereplication/MassBank/\"\n",
    "                            )\n",
    "                            if os.path.exists(sub_dir):\n",
    "                                mbank_files = glob.glob(sub_dir + \"/*proc.csv\")\n",
    "\n",
    "                                for files in mbank_files:\n",
    "                                    if spec_msp[\"id_X\"][mz] in files:\n",
    "                                        mbankproc = pd.read_csv(files)\n",
    "\n",
    "                                        if len(mbankproc) > 0:\n",
    "                                            M_Smiles = mbankproc[\"MBSMILES\"]\n",
    "                                            M_Smiles = list(filter(None, M_Smiles))\n",
    "\n",
    "                                            MB_Mol = []\n",
    "                                            # extract only the InChI of the top 5\n",
    "                                            for j in list(M_Smiles):\n",
    "                                                if not isNaN(j):\n",
    "\n",
    "                                                    mol2 = Chem.MolFromSmiles(j)\n",
    "                                                    MB_Mol.append(mol2)\n",
    "\n",
    "                                            if len(MB_Mol) >= 2:\n",
    "                                                res = rdFMCS.FindMCS(MB_Mol)\n",
    "                                                sm_res = Chem.MolToSmiles(\n",
    "                                                    Chem.MolFromSmarts(res.smartsString)\n",
    "                                                )\n",
    "                                                # if there are atleast 3 heavy atoms in the MCSS, then add it to the result file\n",
    "                                                elem = [\n",
    "                                                    ele\n",
    "                                                    for ele in heavy_atoms\n",
    "                                                    if (ele in sm_res)\n",
    "                                                ]\n",
    "                                                if elem and len(sm_res) >= 3:\n",
    "                                                    spec_msp.loc[\n",
    "                                                        mz, \"MB_MCSSstring\"\n",
    "                                                    ] = res.smartsString\n",
    "                                                    spec_msp.loc[\n",
    "                                                        mz, \"MB_MCSS_SMILES\"\n",
    "                                                    ] = Chem.MolToSmiles(\n",
    "                                                        Chem.MolFromSmarts(\n",
    "                                                            res.smartsString\n",
    "                                                        )\n",
    "                                                    )\n",
    "                    spec_msp.to_csv(specdb_msp_file[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20a460d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCSS_for_SIRIUS(input_dir):\n",
    "\n",
    "\n",
    "    # Describe the heavy atoms to be considered for MCSS\n",
    "    heavy_atoms = [\"C\", \"N\", \"P\", \"O\", \"S\"]\n",
    "    # list all files and directories\n",
    "    for entry in os.listdir(input_dir):\n",
    "        if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "            # for sirius\n",
    "            sirius_msp_csv = input_dir + \"/\" + entry + \"/insilico/MS1DATA.csv\"\n",
    "            sub_dir = input_dir + \"/\" + entry + \"/insilico/SIRIUS/\"\n",
    "            if os.path.exists(sirius_msp_csv) and os.path.exists(sub_dir):\n",
    "                sirius_msp = pd.read_csv(sirius_msp_csv)\n",
    "                sirius_files = glob.glob(sub_dir)\n",
    "                for sir_file in sirius_files:\n",
    "                    r = [s for s in os.listdir(sir_file) if \"structure\" in s]\n",
    "                    for filenames in r:\n",
    "                        sirius_f = sir_file + filenames\n",
    "                        for mz, row in sirius_msp.iterrows():\n",
    "                            if str(sirius_msp[\"id_X\"][mz].split(\"_\")[1]) in sirius_f:\n",
    "                                s_f = pd.read_csv(str(sirius_f))\n",
    "                                if (\n",
    "                                    len(s_f) > 0\n",
    "                                    and \"smiles\" in s_f.columns.values.tolist()\n",
    "                                ):\n",
    "                                    S_Smiles = s_f[\"smiles\"]\n",
    "                                    # create empty list of MB top smiles\n",
    "                                    SIRIUS_Mol = []\n",
    "\n",
    "                                    # extract only the InChI of the top 5\n",
    "                                    for j in list(S_Smiles):\n",
    "                                        mol2 = Chem.MolFromSmiles(j)\n",
    "                                        SIRIUS_Mol.append(mol2)\n",
    "                                    if len(SIRIUS_Mol) >= 2:\n",
    "                                        res = rdFMCS.FindMCS(SIRIUS_Mol)\n",
    "                                        sm_res = Chem.MolToSmiles(\n",
    "                                            Chem.MolFromSmarts(res.smartsString)\n",
    "                                        )\n",
    "                                        # if there are atleast 3 heavy atoms in the MCSS, then add it to the result file\n",
    "                                        elem = [\n",
    "                                            ele\n",
    "                                            for ele in heavy_atoms\n",
    "                                            if (ele in sm_res)\n",
    "                                        ]\n",
    "                                        if elem and len(sm_res) >= 3:\n",
    "                                            sirius_msp.loc[\n",
    "                                                mz, \"SIRIUS_MCSSstring\"\n",
    "                                            ] = res.smartsString\n",
    "                                            sirius_msp.loc[\n",
    "                                                mz, \"SIRIUS_MCSS_SMILES\"\n",
    "                                            ] = Chem.MolToSmiles(\n",
    "                                                Chem.MolFromSmarts(res.smartsString)\n",
    "                                            )\n",
    "                sirius_msp.to_csv(sirius_msp_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3630ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SuspectListScreening(input_dir, SuspectListPath, tanimoto, Source):\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "\n",
    "    SuspectList = pd.read_csv(SuspectListPath)\n",
    "\n",
    "    for entry in os.listdir(input_dir):\n",
    "\n",
    "        if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "\n",
    "            if Source == \"gnps\" or Source == \"specdb\" or Source == \"all\":\n",
    "\n",
    "                sub_dir = input_dir + \"/\" + entry + \"/spectral_dereplication/GNPS/\"\n",
    "                if os.path.exists(sub_dir):\n",
    "                    gnps_files = glob.glob(sub_dir + \"/*proc.csv\")\n",
    "                    for file in gnps_files:\n",
    "                        gnpsproc = pd.read_csv(file)\n",
    "                        if len(gnpsproc) > 0:\n",
    "                            for g, row in gnpsproc.iterrows():\n",
    "                                for s, row in SuspectList.iterrows():\n",
    "                                    if (\n",
    "                                        not isNaN(gnpsproc[\"GNPSSMILES\"][g])\n",
    "                                        and gnpsproc[\"GNPSSMILES\"][g] != \" \"\n",
    "                                    ):\n",
    "                                        if (\n",
    "                                            not isNaN(SuspectList[\"SMILES\"][s])\n",
    "                                            and SuspectList[\"SMILES\"][s] != \" \"\n",
    "                                        ):\n",
    "                                            LHms2 = [\n",
    "                                                Chem.MolFromSmiles(\n",
    "                                                    gnpsproc[\"GNPSSMILES\"][g]\n",
    "                                                ),\n",
    "                                                Chem.MolFromSmiles(\n",
    "                                                    SuspectList[\"SMILES\"][s]\n",
    "                                                ),\n",
    "                                            ]\n",
    "                                            LHfps2 = [\n",
    "                                                AllChem.GetMorganFingerprintAsBitVect(\n",
    "                                                    x2, 2, nBits=2048\n",
    "                                                )\n",
    "                                                for x2 in LHms2\n",
    "                                            ]\n",
    "                                            LHtn2 = DataStructs.FingerprintSimilarity(\n",
    "                                                LHfps2[0], LHfps2[1]\n",
    "                                            )\n",
    "                                            if LHtn2 >= tanimoto:\n",
    "                                                gnpsproc.loc[\n",
    "                                                    g, \"SLGsmiles\"\n",
    "                                                ] = SuspectList[\"SMILES\"][s]\n",
    "                                                gnpsproc.loc[\n",
    "                                                    g, \"SLGname\"\n",
    "                                                ] = SuspectList[\"Name\"][s]\n",
    "                                                gnpsproc.loc[g, \"SLGtanimoto\"] = LHtn2\n",
    "                        gnpsproc.to_csv(file)\n",
    "                        return gnpsproc\n",
    "            if Source == \"hmdb\" or Source == \"specdb\" or Source == \"all\":\n",
    "\n",
    "                sub_dir = input_dir + \"/\" + entry + \"/spectral_dereplication/HMDB/\"\n",
    "                if os.path.exists(sub_dir):\n",
    "                    hmdb_files = glob.glob(sub_dir + \"/*proc.csv\")\n",
    "                    for file in hmdb_files:\n",
    "\n",
    "                        hmdbproc = pd.read_csv(file)\n",
    "                        if len(hmdbproc) > 0:\n",
    "                            for h, row in hmdbproc.iterrows():\n",
    "                                for s, row in SuspectList.iterrows():\n",
    "                                    if (\n",
    "                                        not isNaN(hmdbproc[\"HMDBSMILES\"][h])\n",
    "                                        and hmdbproc[\"HMDBSMILES\"][h] != \" \"\n",
    "                                    ):\n",
    "                                        if (\n",
    "                                            not isNaN(SuspectList[\"SMILES\"][s])\n",
    "                                            and SuspectList[\"SMILES\"][s] != \" \"\n",
    "                                        ):\n",
    "                                            LHms2 = [\n",
    "                                                Chem.MolFromSmiles(\n",
    "                                                    hmdbproc[\"HMDBSMILES\"][h]\n",
    "                                                ),\n",
    "                                                Chem.MolFromSmiles(\n",
    "                                                    SuspectList[\"SMILES\"][s]\n",
    "                                                ),\n",
    "                                            ]\n",
    "                                            LHfps2 = [\n",
    "                                                AllChem.GetMorganFingerprintAsBitVect(\n",
    "                                                    x2, 2, nBits=2048\n",
    "                                                )\n",
    "                                                for x2 in LHms2\n",
    "                                            ]\n",
    "                                            LHtn2 = DataStructs.FingerprintSimilarity(\n",
    "                                                LHfps2[0], LHfps2[1]\n",
    "                                            )\n",
    "                                            if LHtn2 >= tanimoto:\n",
    "                                                hmdbproc.loc[\n",
    "                                                    h, \"SLHsmiles\"\n",
    "                                                ] = SuspectList[\"SMILES\"][s]\n",
    "                                                hmdbproc.loc[\n",
    "                                                    h, \"SLHname\"\n",
    "                                                ] = SuspectList[\"Name\"][s]\n",
    "                                                hmdbproc.loc[h, \"SLHtanimoto\"] = LHtn2\n",
    "\n",
    "                        hmdbproc.to_csv(file)\n",
    "                        return hmdbproc\n",
    "\n",
    "            if Source == \"mbank\" or Source == \"specdb\" or Source == \"all\":\n",
    "\n",
    "                sub_dir = input_dir + \"/\" + entry + \"/spectral_dereplication/MassBank/\"\n",
    "                if os.path.exists(sub_dir):\n",
    "                    mbank_files = glob.glob(sub_dir + \"/*proc.csv\")\n",
    "                    for file in mbank_files:\n",
    "\n",
    "                        mbankproc = pd.read_csv(file)\n",
    "\n",
    "                        if len(mbankproc) > 0:\n",
    "                            for m, row in mbankproc.iterrows():\n",
    "                                for s, row in SuspectList.iterrows():\n",
    "                                    if (\n",
    "                                        not isNaN(mbankproc[\"MBSMILES\"][m])\n",
    "                                        and mbankproc[\"MBSMILES\"][m] != \" \"\n",
    "                                    ):\n",
    "                                        if (\n",
    "                                            not isNaN(SuspectList[\"SMILES\"][s])\n",
    "                                            and SuspectList[\"SMILES\"][s] != \" \"\n",
    "                                        ):\n",
    "                                            LHms2 = [\n",
    "                                                Chem.MolFromSmiles(\n",
    "                                                    mbankproc[\"MBSMILES\"][m]\n",
    "                                                ),\n",
    "                                                Chem.MolFromSmiles(\n",
    "                                                    SuspectList[\"SMILES\"][s]\n",
    "                                                ),\n",
    "                                            ]\n",
    "                                            LHfps2 = [\n",
    "                                                AllChem.GetMorganFingerprintAsBitVect(\n",
    "                                                    x2, 2, nBits=2048\n",
    "                                                )\n",
    "                                                for x2 in LHms2\n",
    "                                            ]\n",
    "                                            LHtn2 = DataStructs.FingerprintSimilarity(\n",
    "                                                LHfps2[0], LHfps2[1]\n",
    "                                            )\n",
    "                                            if LHtn2 >= tanimoto:\n",
    "                                                mbankproc.loc[\n",
    "                                                    m, \"SLMsmiles\"\n",
    "                                                ] = SuspectList[\"SMILES\"][s]\n",
    "                                                mbankproc.loc[\n",
    "                                                    m, \"SLMname\"\n",
    "                                                ] = SuspectList[\"Name\"][s]\n",
    "                                                mbankproc.loc[m, \"SLMtanimoto\"] = LHtn2\n",
    "\n",
    "                        mbankproc.to_csv(file)\n",
    "                        return mbankproc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5345e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate Selection\n",
    "def chemMN_CandidateSelection(df, tn_sim=0.85):\n",
    "\n",
    "    \"\"\"chemMN_CandidateSelection function is used to generate a Cytoscape readable tsv file.\n",
    "    This file contains start(starting SMILES) and end(target SMILES) nodes and the tanimoto\n",
    "    similarity scores between the nodes. User can visualize the structural similarity\n",
    "    between the given SMILES. It provides an \"ALL against ALL\" network.\n",
    "\n",
    "    Parameters:\n",
    "    df: dataframe that contains \"SMILES\", \"ranks\", \"Source\". This function is specifically for\n",
    "    candidate selection and so these columns are necessary.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    dataframe: it returns a df with follwoing columns to be loaded into Cytoscape.\n",
    "    1. Start, starting node/SMILES\n",
    "    2. End, ending node/SMILES\n",
    "    3. Tanimoto, Tanimoto between Start and End node\n",
    "    4. Start_SMILES\n",
    "    5. End_SMILES\n",
    "    6. Start_Source\n",
    "    7. End_Source\n",
    "    8. MCSS, Maximum Common Substructure between start and end node/SMILES\n",
    "    9. sorted_row, contains ids of the start and end nodes as a list\n",
    "\n",
    "\n",
    "    Usage:\n",
    "    chemMN_CandidateSelection(df)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # define an empty variable\n",
    "    # one_df = []\n",
    "    # define empty variable to save the edges\n",
    "    dbn = []\n",
    "    # for each entry in the df\n",
    "    for i, row in df.iterrows():\n",
    "        # to compare each element with each other element of the df\n",
    "        for j, row in df.iterrows():\n",
    "            try:\n",
    "                # calcultae tanimoto\n",
    "                ms = [\n",
    "                    Chem.MolFromSmiles(df[\"SMILES\"][i]),\n",
    "                    Chem.MolFromSmiles(df[\"SMILES\"][j]),\n",
    "                ]\n",
    "                fps = [\n",
    "                    AllChem.GetMorganFingerprintAsBitVect(x, 2, nBits=2048) for x in ms\n",
    "                ]\n",
    "                tn = DataStructs.FingerprintSimilarity(fps[0], fps[1])\n",
    "\n",
    "                # save all entries to a matrix\n",
    "                dbn.append(\n",
    "                    {\n",
    "                        \"Name_i\": df[\"ranks\"][i],\n",
    "                        \"Name_j\": df[\"ranks\"][j],\n",
    "                        \"i\": df[\"SMILES\"][i],\n",
    "                        \"j\": df[\"SMILES\"][j],\n",
    "                        \"Source_i\": df[\"Source\"][i],\n",
    "                        \"Source_j\": df[\"Source\"][j],\n",
    "                        \"Tanimoto\": tn,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            except Exception:\n",
    "                # print(e.string)\n",
    "                pass\n",
    "\n",
    "    # save chemical similarities\n",
    "    db_edgenode = pd.DataFrame(dbn)\n",
    "\n",
    "    # another empty variable to store the results for final tsv file\n",
    "    dfe = []\n",
    "\n",
    "    # heavy atoms for MCSS Calculation\n",
    "    heavy_atoms = [\"C\", \"N\", \"P\", \"O\", \"S\"]\n",
    "\n",
    "    # for the previous dataframe\n",
    "    for i, row in db_edgenode.iterrows():\n",
    "        # if the tanimoto > 0.85 for high similarity\n",
    "        if db_edgenode[\"Tanimoto\"][i] >= tn:\n",
    "\n",
    "            # calculate MCSS\n",
    "            n = [\n",
    "                Chem.MolFromSmiles(db_edgenode[\"i\"][i]),\n",
    "                Chem.MolFromSmiles(db_edgenode[\"j\"][i]),\n",
    "            ]\n",
    "            res = rdFMCS.FindMCS(n)\n",
    "            sm_res = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "\n",
    "            # Check if the MCSS has one of the heavy atoms and whether they are\n",
    "            # more than 3\n",
    "            elem = [ele for ele in heavy_atoms if (ele in sm_res)]\n",
    "            if elem and len(sm_res) >= 3:\n",
    "                MCSS_SMILES = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "\n",
    "            # save everything into a dataframe\n",
    "            dfe.append(\n",
    "                {\n",
    "                    \"Start\": db_edgenode[\"Name_i\"][i],\n",
    "                    \"End\": db_edgenode[\"Name_j\"][i],\n",
    "                    \"Tanimoto\": db_edgenode[\"Tanimoto\"][i],\n",
    "                    \"Start_SMILES\": db_edgenode[\"i\"][i],\n",
    "                    \"End_SMILES\": db_edgenode[\"j\"][i],\n",
    "                    \"Start_Source\": db_edgenode[\"Source_i\"][i],\n",
    "                    \"End_Source\": db_edgenode[\"Source_j\"][i],\n",
    "                    \"MCSS\": MCSS_SMILES,\n",
    "                }\n",
    "            )\n",
    "    df_edge = pd.DataFrame(dfe)\n",
    "    # generate a column called sorted_row which contains ids of the start and end nodes as a list\n",
    "    df_edge[\"Start\"] = df_edge[\"Start\"].astype(str)\n",
    "    df_edge[\"End\"] = df_edge[\"End\"].astype(str)\n",
    "    df_edge[\"sorted_row\"] = [sorted([a, b]) for a, b in zip(df_edge.Start, df_edge.End)]\n",
    "    df_edge[\"sorted_row\"] = df_edge[\"sorted_row\"].astype(str)\n",
    "    df_edge.drop_duplicates(subset=[\"sorted_row\"], inplace=True)\n",
    "\n",
    "    return df_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bcdab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_candidate_selection(\n",
    "    df,\n",
    "    Source=\"SGHM\",\n",
    "    tn_ident=0.99,\n",
    "    sirius_df=None,\n",
    "    mbank_df=None,\n",
    "    gnps_df=None,\n",
    "    hmdb_df=None,\n",
    "):\n",
    "\n",
    "    \"\"\"one_candidate_selection function is used to generate a dataframe that tells,\n",
    "    for each candidate SMILES, what was the source or how many sources had the same\n",
    "    candidate. The idea is to merge all candidate SMILES into one list, preserving\n",
    "    the rank and source, and then checking whether these SMILES come from SIRIUS or\n",
    "    any spectral DB. If a SMILE is repeated in more sources, its confidence score\n",
    "    increases and is considered the most likely candidate structure. This function\n",
    "    is not stand-alone and is part of the function CandidateSelection_SimilarityandIdentity\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    df: dataframe that contains \"SMILES\", \"ranks\", \"Source\". This function is\n",
    "    specifically for candidate selection and so these columns are necessary.\n",
    "    Source: this depends on how many sources were used. Possiblilities are:\n",
    "    1. SGHM (all)\n",
    "    2. SGM (SIRIUS, GNPS, MassBank)\n",
    "    3. SHM (SIRIUS, HMDB, MassBank)\n",
    "    4. SGH (SIRIUS, GNPS, HMDB)\n",
    "    5. GHM (GNPS, HMDB, MassBank)\n",
    "    6. SG (SIRIUS, GNPS)\n",
    "    7. SH (SIRIUS, HMDB)\n",
    "    8. SM (SIRIUS, MassBank)\n",
    "    9. GM (GNPS, MassBank)\n",
    "    10. GH (GNPS, HMDB)\n",
    "    11. HM (HMDB, MassBank)\n",
    "    12. S\n",
    "    13. G\n",
    "    14. H\n",
    "    15. M\n",
    "\n",
    "    Returns:\n",
    "    dataframe: it returns a df with follwoing columns which can be used to\n",
    "    prioritize a database for the final candidate selection.\n",
    "    1. Source, contains name of the source (SIRIUS, GNPS, HMDB or MassBank)\n",
    "    2. ranks, contains first letter of the source and a rank number seperated\n",
    "    by _ e.g: G_1(GNPS, 1st rank)\n",
    "    3. SMILES\n",
    "    4. SIRIUS, the rank again but only when the corresponding row SMILES is\n",
    "    also part of SIRIUS results\n",
    "    5. GNPS , same as SIRIUS but for GNPS\n",
    "    5. MassBank\n",
    "    6. HMDB\n",
    "\n",
    "\n",
    "    Usage:\n",
    "    chemMN_CandidateSelection(df, Source = \"SGHM\")\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # define empty columns for each Source to only fill if the corresponding\n",
    "    # SMILES is also present in the source\n",
    "\n",
    "    df[\"SIRIUS\"] = np.nan\n",
    "    df[\"GNPS\"] = np.nan\n",
    "    df[\"MassBank\"] = np.nan\n",
    "    df[\"HMDB\"] = np.nan\n",
    "\n",
    "    # for each SMILES in df\n",
    "    for smiles, rows in df.iterrows():\n",
    "\n",
    "        # If the source contains SIRIUS\n",
    "        if (\n",
    "            Source == \"SGHM\"\n",
    "            or Source == \"SGM\"\n",
    "            or Source == \"SGH\"\n",
    "            or Source == \"SHM\"\n",
    "            or Source == \"SG\"\n",
    "            or Source == \"SM\"\n",
    "            or Source == \"SH\"\n",
    "            or Source == \"S\"\n",
    "        ):\n",
    "            # sirius_df comes from within the function CandidateSelection_SimilarityandIdentity\n",
    "            for sirius_i, row in sirius_df.iterrows():\n",
    "                # calculate tanimoto\n",
    "                try:\n",
    "                    ms = [\n",
    "                        Chem.MolFromSmiles(df[\"SMILES\"][smiles]),\n",
    "                        Chem.MolFromSmiles(sirius_df[\"smiles\"][sirius_i]),\n",
    "                    ]\n",
    "                    fps = [\n",
    "                        AllChem.GetMorganFingerprintAsBitVect(x, 2, nBits=2048)\n",
    "                        for x in ms\n",
    "                    ]\n",
    "                    tn = DataStructs.FingerprintSimilarity(fps[0], fps[1])\n",
    "                    # since we are dealing with idenity here so tanimoto of 0.99 is appropriate\n",
    "                    if tn >= tn_ident:\n",
    "\n",
    "                        # if SIRIUS is blank, add the SIRIUS id\n",
    "                        if isNaN(df[\"SIRIUS\"][smiles]):\n",
    "\n",
    "                            df.loc[smiles, \"SIRIUS\"] = sirius_df[\"rank_ids\"][sirius_i]\n",
    "                        # if not empty, add SIRIUS id, with a comma\n",
    "                        else:\n",
    "                            df.loc[smiles, \"SIRIUS\"] = (\n",
    "                                str(df[\"SIRIUS\"][smiles])\n",
    "                                + \", \"\n",
    "                                + sirius_df[\"rank_ids\"][sirius_i]\n",
    "                            )\n",
    "\n",
    "                except Exception:\n",
    "                    # print(e.string)\n",
    "                    pass\n",
    "\n",
    "        # If the Source contains GNPS\n",
    "        if (\n",
    "            Source == \"SGHM\"\n",
    "            or Source == \"SGM\"\n",
    "            or Source == \"SGH\"\n",
    "            or Source == \"GHM\"\n",
    "            or Source == \"SG\"\n",
    "            or Source == \"GM\"\n",
    "            or Source == \"GH\"\n",
    "            or Source == \"G\"\n",
    "        ):\n",
    "\n",
    "            # gnps_df comes from within the function CandidateSelection_SimilarityandIdentity\n",
    "            for gnps_i, row in gnps_df.iterrows():\n",
    "                try:\n",
    "                    # calculate tanimoto\n",
    "                    ms = [\n",
    "                        Chem.MolFromSmiles(df[\"SMILES\"][smiles]),\n",
    "                        Chem.MolFromSmiles(gnps_df[\"GNPSSMILES\"][gnps_i]),\n",
    "                    ]\n",
    "                    fps = [\n",
    "                        AllChem.GetMorganFingerprintAsBitVect(x, 2, nBits=2048)\n",
    "                        for x in ms\n",
    "                    ]\n",
    "                    tn = DataStructs.FingerprintSimilarity(fps[0], fps[1])\n",
    "\n",
    "                    # since we are dealing with idenity here so tanimoto of 0.99 is appropriate\n",
    "                    if tn >= tn_ident:\n",
    "\n",
    "                        # if GNPS is blank, add the GNPS id\n",
    "                        if isNaN(df[\"GNPS\"][smiles]):\n",
    "\n",
    "                            df.loc[smiles, \"GNPS\"] = gnps_df[\"rank_ids\"][gnps_i]\n",
    "                        # if not empty, add GNPS id, with a comma\n",
    "                        else:\n",
    "                            df.loc[smiles, \"GNPS\"] = (\n",
    "                                str(df[\"GNPS\"][smiles])\n",
    "                                + \", \"\n",
    "                                + gnps_df[\"rank_ids\"][gnps_i]\n",
    "                            )\n",
    "\n",
    "                except Exception:\n",
    "                    # print(e.string)\n",
    "                    pass\n",
    "\n",
    "        # If the source contains HMDB\n",
    "        if (\n",
    "            Source == \"SGHM\"\n",
    "            or Source == \"SGM\"\n",
    "            or Source == \"SHM\"\n",
    "            or Source == \"GHM\"\n",
    "            or Source == \"SM\"\n",
    "            or Source == \"GM\"\n",
    "            or Source == \"HM\"\n",
    "            or Source == \"H\"\n",
    "        ):\n",
    "            for hmdb_i, row in hmdb_df.iterrows():\n",
    "                try:\n",
    "                    # calculate tanimoto\n",
    "                    ms = [\n",
    "                        Chem.MolFromSmiles(df[\"SMILES\"][smiles]),\n",
    "                        Chem.MolFromSmiles(hmdb_df[\"HMDBSMILES\"][hmdb_i]),\n",
    "                    ]\n",
    "                    fps = [\n",
    "                        AllChem.GetMorganFingerprintAsBitVect(x, 2, nBits=2048)\n",
    "                        for x in ms\n",
    "                    ]\n",
    "                    tn = DataStructs.FingerprintSimilarity(fps[0], fps[1])\n",
    "\n",
    "                    # since we are dealing with idenity here so tanimoto of 0.99 is appropriate\n",
    "                    if tn >= tn_ident:\n",
    "\n",
    "                        # if HMDB is blank, add the HMDB id\n",
    "                        if isNaN(df[\"HMDB\"][smiles]):\n",
    "\n",
    "                            df.loc[smiles, \"HMDB\"] = hmdb_df[\"rank_ids\"][hmdb_i]\n",
    "                        # if not empty, add HMDB id, with a comma\n",
    "                        else:\n",
    "                            df.loc[smiles, \"HMDB\"] = (\n",
    "                                str(df[\"HMDB\"][smiles])\n",
    "                                + \", \"\n",
    "                                + hmdb_df[\"rank_ids\"][hmdb_i]\n",
    "                            )\n",
    "\n",
    "                except Exception:\n",
    "                    # print(e.string)\n",
    "                    pass\n",
    "\n",
    "\n",
    "        # If the source contains MassBank\n",
    "        if (\n",
    "            Source == \"SGHM\"\n",
    "            or Source == \"SGH\"\n",
    "            or Source == \"SHM\"\n",
    "            or Source == \"GHM\"\n",
    "            or Source == \"SH\"\n",
    "            or Source == \"GH\"\n",
    "            or Source == \"HM\"\n",
    "            or Source == \"M\"\n",
    "        ):\n",
    "            # mbank_df comes from within the function CandidateSelection_SimilarityandIdentity\n",
    "            for mbank_i, row in mbank_df.iterrows():\n",
    "                try:\n",
    "                    # calculate tanimoto\n",
    "                    ms = [\n",
    "                        Chem.MolFromSmiles(df[\"SMILES\"][smiles]),\n",
    "                        Chem.MolFromSmiles(mbank_df[\"MBSMILES\"][mbank_i]),\n",
    "                    ]\n",
    "                    fps = [\n",
    "                        AllChem.GetMorganFingerprintAsBitVect(x, 2, nBits=2048)\n",
    "                        for x in ms\n",
    "                    ]\n",
    "                    tn = DataStructs.FingerprintSimilarity(fps[0], fps[1])\n",
    "\n",
    "                    # since we are dealing with idenity here so tanimoto of 0.99 is appropriate\n",
    "                    if tn >= tn_ident:\n",
    "\n",
    "                        # if MassBank is blank, add the MassBank id\n",
    "                        if isNaN(df[\"MassBank\"][smiles]):\n",
    "\n",
    "                            df.loc[smiles, \"MassBank\"] = mbank_df[\"rank_ids\"][mbank_i]\n",
    "                        # if not empty, add MassBank id, with a comma\n",
    "                        else:\n",
    "                            df.loc[smiles, \"MassBank\"] = (\n",
    "                                str(df[\"MassBank\"][smiles])\n",
    "                                + \", \"\n",
    "                                + mbank_df[\"rank_ids\"][mbank_i]\n",
    "                            )\n",
    "\n",
    "                except Exception:\n",
    "                    # print(e.string)\n",
    "                    pass\n",
    "\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33c4ab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_count_column(df_one_candidate):\n",
    "    # create new df only with the Sources column\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"SIRIUS\": df_one_candidate[\"SIRIUS\"],\n",
    "            \"GNPS\": df_one_candidate[\"GNPS\"],\n",
    "            \"MassBank\": df_one_candidate[\"MassBank\"],\n",
    "            \"HMDB\": df_one_candidate[\"HMDB\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # now check which rows have a value\n",
    "    df = df.dropna(axis=0, how=\"all\")\n",
    "\n",
    "    # df_one_candidate = df_one_candidate.dropna(subset=[\"SIRIUS\", \"GNPS\", \"HMDB\", \"MassBank\"], how='all', inplace=True)\n",
    "\n",
    "    index_SIRIUS = [x for x, row in df.iterrows() if not isNaN(df[\"SIRIUS\"][x])]\n",
    "    index_GNPS = [x for x, row in df.iterrows() if not isNaN(df[\"GNPS\"][x])]\n",
    "    index_MassBank = [x for x, row in df.iterrows() if not isNaN(df[\"MassBank\"][x])]\n",
    "    index_HMDB = [x for x, row in df.iterrows() if not isNaN(df[\"HMDB\"][x])]\n",
    "\n",
    "    # make a list of the rows\n",
    "    list_of_indices = index_SIRIUS + index_GNPS + index_MassBank + index_HMDB\n",
    "\n",
    "    # count how mnay times one of the rows is appearing and add count\n",
    "    count_list = [[x, list_of_indices.count(x)] for x in set(list_of_indices)]\n",
    "    # add this info to one_can\n",
    "    df_one_candidate[\"Count\"] = [count_list[x][1] for x in range(len(count_list))]\n",
    "    # sort the list by count in descending order\n",
    "    sorted_count_one_candidate = df_one_candidate.sort_values(\n",
    "        by=\"Count\", ascending=False\n",
    "    )\n",
    "    return sorted_count_one_candidate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b8f9a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sources_1(candidates_with_counts, merged_df, mer, sirius_df):\n",
    "    \"\"\"if only 1 source has confirmed the presence of a certain SMILES.\n",
    "    This holds true when each candidate SMILES has only one source. The\n",
    "    function selects the best candidate\n",
    "\n",
    "    Parameters:\n",
    "    candidates_with_counts: this is the result from the function add_count_column\n",
    "    and contains a ordered dataframe, with the most sourced SMILES at top.\n",
    "    merged_df: dataframe that contains all features from the input mzML file\n",
    "\n",
    "    Returns:\n",
    "    merged_df: with added top SMILES, Annotation Sources, Annotation Count, and\n",
    "    MSI-Level\n",
    "\n",
    "    Usage:\n",
    "    sources_1(candidates_with_counts, merged_df)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df_count_1 = candidates_with_counts[candidates_with_counts[\"Count\"] == 1]\n",
    "    # rank number e.g:1\n",
    "    df_count_1[\"rank_num\"] = [counts.split(\"_\")[1] for counts in df_count_1[\"ranks\"]]\n",
    "    df_count_1[\"rank_num\"] = [int(x) for x in df_count_1[\"rank_num\"]]\n",
    "\n",
    "    # if only one unique source e.g: only SIRIUS gave out candidates\n",
    "    if len(np.unique(df_count_1[\"Source\"])) == 1:\n",
    "\n",
    "        df_count_1 = df_count_1.sort_values(by=\"rank_num\")\n",
    "\n",
    "        df_count_1 = df_count_1[df_count_1[\"rank_num\"] == 1]\n",
    "\n",
    "        df_count_1[\"count_min\"] = [\n",
    "            str(df_count_1[\"SIRIUS\"][x])\n",
    "            + str(df_count_1[\"GNPS\"][x])\n",
    "            + str(df_count_1[\"MassBank\"][x])\n",
    "            + str(df_count_1[\"HMDB\"][x])\n",
    "            for x, row in df_count_1.iterrows()\n",
    "        ]\n",
    "\n",
    "        df_count_1[\"count_max\"] = [x.count(\"_1\") for x in df_count_1[\"count_min\"]]\n",
    "\n",
    "        df_count_1 = df_count_1.sort_values(by=\"count_max\", ascending=False)\n",
    "\n",
    "        df_count_1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        #  choose the index/row from df_count_1 that contains priority wise databases\n",
    "        if \"GNPS\" in list(df_count_1[\"Source\"]):\n",
    "            can1 = df_count_1.index[df_count_1[\"Source\"].str.contains(\"GNPS\")].tolist()\n",
    "            merged_df.loc[mer, \"SMILES\"] = list(df_count_1[\"SMILES\"][can1])[0]\n",
    "            comp = pcp.get_compounds(merged_df[\"SMILES\"][mer], 'smiles')\n",
    "            try:\n",
    "                if comp:\n",
    "                    for c in comp:\n",
    "                        merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                        merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                        merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "            except Exception:\n",
    "                pass\n",
    "            merged_df[\"superclass\"][mer] = np.nan\n",
    "            merged_df[\"class\"][mer] = np.nan\n",
    "            merged_df[\"subclass\"][mer] = np.nan\n",
    "            merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "        elif \"SIRIUS\" in list(df_count_1[\"Source\"]):\n",
    "            can1 = df_count_1.index[\n",
    "                df_count_1[\"Source\"].str.contains(\"SIRIUS\")\n",
    "            ].tolist()\n",
    "            merged_df.loc[mer, \"SMILES\"] = list(df_count_1[\"SMILES\"][can1])[0]\n",
    "            merged_df[\"Formula\"] = sirius_df[\"molecularFormula\"][0]\n",
    "            merged_df[\"superclass\"] = sirius_df[\"superclass\"][0]\n",
    "            merged_df[\"class\"] = sirius_df[\"class\"][0]\n",
    "            merged_df[\"subclass\"] = sirius_df[\"subclass\"][0]\n",
    "            merged_df[\"ClassificationSource\"] = \"CANOPUS\"\n",
    "        \n",
    "        elif \"MassBank\" in list(df_count_1[\"Source\"]):\n",
    "            can1 = df_count_1.index[\n",
    "                df_count_1[\"Source\"].str.contains(\"MassBank\")\n",
    "            ].tolist()\n",
    "            merged_df.loc[mer, \"SMILES\"] = list(df_count_1[\"SMILES\"][can1])[0]\n",
    "            comp = pcp.get_compounds(merged_df[\"SMILES\"][mer], 'smiles')\n",
    "            try:\n",
    "                if comp:\n",
    "                    for c in comp:\n",
    "                        merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                        merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                        merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "            except Exception:\n",
    "                pass\n",
    "            merged_df[\"superclass\"][mer] = np.nan\n",
    "            merged_df[\"class\"][mer] = np.nan\n",
    "            merged_df[\"subclass\"][mer] = np.nan\n",
    "            merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "        elif \"HMDB\" in list(df_count_1[\"Source\"]):\n",
    "            can1 = df_count_1.index[df_count_1[\"Source\"].str.contains(\"HMDB\")].tolist()\n",
    "            merged_df.loc[mer, \"SMILES\"] = list(df_count_1[\"SMILES\"][can1])[0]\n",
    "            comp = pcp.get_compounds(merged_df[\"SMILES\"][mer], 'smiles')\n",
    "            try:\n",
    "                if comp:\n",
    "                    for c in comp:\n",
    "                        merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                        merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                        merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "            except Exception:\n",
    "                pass\n",
    "            merged_df[\"superclass\"][mer] = np.nan\n",
    "            merged_df[\"class\"][mer] = np.nan\n",
    "            merged_df[\"subclass\"][mer] = np.nan\n",
    "            merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        merged_df.loc[mer, \"AnnotationCount\"] = list(df_count_1[\"Count\"][can1])[0]\n",
    "\n",
    "        gnps_indices = list(df_count_1[(df_count_1[\"GNPS\"].notnull())].index)\n",
    "        mbank_indices = list(df_count_1[(df_count_1[\"MassBank\"].notnull())].index)\n",
    "        hmdb_indices = list(df_count_1[(df_count_1[\"HMDB\"].notnull())].index)\n",
    "        sirius_indices = list(df_count_1[(df_count_1[\"SIRIUS\"].notnull())].index)\n",
    "\n",
    "        if list(can1)[0] in sirius_indices:\n",
    "            merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "                str(merged_df[\"AnnotationSources\"][mer]) + \"|SIRIUS\"\n",
    "            )\n",
    "        if list(can1)[0] in mbank_indices:\n",
    "            merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "                str(merged_df[\"AnnotationSources\"][mer]) + \"|MassBank\"\n",
    "            )\n",
    "            # print(\"mbank\")\n",
    "        if list(can1)[0] in hmdb_indices:\n",
    "            merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "                str(merged_df[\"AnnotationSources\"][mer]) + \"|HMDB\"\n",
    "            )\n",
    "            # print(\"hmdb\")\n",
    "        if list(can1)[0] in gnps_indices:\n",
    "            merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "                str(merged_df[\"AnnotationSources\"][mer]) + \"|GNPS\"\n",
    "            )\n",
    "            # print(\"gnps\")\n",
    "        \n",
    "        if \"nan|SIRIUS\" == merged_df[\"AnnotationSources\"][mer]:\n",
    "            merged_df.loc[mer, \"MSILevel\"] = 3\n",
    "\n",
    "        merged_df[\"AnnotationSources\"][mer] = merged_df[\"AnnotationSources\"][\n",
    "            mer\n",
    "        ].replace(\"nan|\", \"\")\n",
    "\n",
    "        if (\n",
    "            \"HMDB\" in merged_df[\"AnnotationSources\"][mer]\n",
    "            or \"GNPS\" in merged_df[\"AnnotationSources\"][mer]\n",
    "            or \"MassBank\" in merged_df[\"AnnotationSources\"][mer]\n",
    "        ):\n",
    "            merged_df.loc[mer, \"MSILevel\"] = 2\n",
    "        \n",
    "    # if only two or more unique source\n",
    "    else:\n",
    "\n",
    "        df_count_1 = candidates_with_counts[candidates_with_counts[\"Count\"] == 1]\n",
    "        df_count_1[\"rank_num\"] = [\n",
    "            counts.split(\"_\")[1] for counts in df_count_1[\"ranks\"]\n",
    "        ]\n",
    "        df_count_1[\"rank_num\"] = [int(x) for x in df_count_1[\"rank_num\"]]\n",
    "        df_count_1[\"count_min\"] = [\n",
    "            str(df_count_1[\"SIRIUS\"][x])\n",
    "            + str(df_count_1[\"GNPS\"][x])\n",
    "            + str(df_count_1[\"MassBank\"][x])\n",
    "            + str(df_count_1[\"HMDB\"][x])\n",
    "            for x, row in df_count_1.iterrows()\n",
    "        ]\n",
    "        lengths = df_count_1[\"count_min\"].str.len()\n",
    "        \n",
    "        if len(np.unique(lengths)) > 1:\n",
    "            argmax = np.where(lengths == lengths.max())[0]\n",
    "            merged_df.loc[mer, \"SMILES\"] = df_count_1[\"SMILES\"][argmax[0]]\n",
    "            merged_df.loc[mer, \"AnnotationCount\"] = df_count_1[\"Count\"][argmax[0]]\n",
    "            df_count_na = df_count_1.notna()\n",
    "            \n",
    "            list_sources = [*filter(df_count_na.loc[7].get, df_count_na.loc[7].index)]\n",
    "            \n",
    "            if \"GNPS\" in list_sources:\n",
    "                merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "                    str(merged_df[\"AnnotationSources\"][mer]) + \"|GNPS\"\n",
    "                )\n",
    "                comp = pcp.get_compounds(merged_df[\"SMILES\"][mer], 'smiles')\n",
    "                try:\n",
    "                    if comp:\n",
    "                        for c in comp:\n",
    "                            merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                            merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                            merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "                except Exception:\n",
    "                    pass\n",
    "                merged_df[\"superclass\"][mer] = np.nan\n",
    "                merged_df[\"class\"][mer] = np.nan\n",
    "                merged_df[\"subclass\"][mer] = np.nan\n",
    "                merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "\n",
    "            \n",
    "            elif \"SIRIUS\" in list_sources:\n",
    "                merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "                    str(merged_df[\"AnnotationSources\"][mer]) + \"|SIRIUS\"\n",
    "                )\n",
    "                merged_df[\"Formula\"] = sirius_df[\"molecularFormula\"][0]\n",
    "                merged_df[\"superclass\"] = sirius_df[\"superclass\"][0]\n",
    "                merged_df[\"class\"] = sirius_df[\"class\"][0]\n",
    "                merged_df[\"subclass\"] = sirius_df[\"subclass\"][0]\n",
    "                merged_df[\"ClassificationSource\"] = \"CANOPUS\"\n",
    "                \n",
    "            elif \"MassBank\" in list_sources:\n",
    "                merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "                    str(merged_df[\"AnnotationSources\"][mer]) + \"|Massbank\"\n",
    "                )\n",
    "                comp = pcp.get_compounds(merged_df[\"SMILES\"][mer], 'smiles')\n",
    "                try:\n",
    "                    if comp:\n",
    "                        for c in comp:\n",
    "                            merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                            merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                            merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "                except Exception:\n",
    "                    pass\n",
    "                merged_df[\"superclass\"][mer] = np.nan\n",
    "                merged_df[\"class\"][mer] = np.nan\n",
    "                merged_df[\"subclass\"][mer] = np.nan\n",
    "                merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "\n",
    "            elif \"HMDB\" in list_sources:\n",
    "                merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "                    str(merged_df[\"AnnotationSources\"][mer]) + \"|HMDB\"\n",
    "                )\n",
    "                comp = pcp.get_compounds(merged_df[\"SMILES\"][mer], 'smiles')\n",
    "                try:\n",
    "                    if comp:\n",
    "                        for c in comp:\n",
    "                            merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                            merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                            merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "                except Exception:\n",
    "                    pass\n",
    "                merged_df[\"superclass\"][mer] = np.nan\n",
    "                merged_df[\"class\"][mer] = np.nan\n",
    "                merged_df[\"subclass\"][mer] = np.nan\n",
    "                merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            merged_df[\"AnnotationSources\"][mer] = merged_df[\"AnnotationSources\"][\n",
    "                mer\n",
    "            ].replace(\"nan|\", \"\")\n",
    "            \n",
    "            \n",
    "            if (\n",
    "                \"HMDB\" in merged_df[\"AnnotationSources\"][mer]\n",
    "                or \"GNPS\" in merged_df[\"AnnotationSources\"][mer]\n",
    "                or \"MassBank\" in merged_df[\"AnnotationSources\"][mer]\n",
    "            ):\n",
    "                merged_df.loc[mer, \"MSILevel\"] = 2\n",
    "                \n",
    "            if \"SIRIUS\" == merged_df[\"AnnotationSources\"][mer]:\n",
    "                merged_df.loc[mer, \"MSILevel\"] = 3\n",
    "        else:\n",
    "            \n",
    "            if \"GNPS\" in np.unique(df_count_1[\"Source\"]):\n",
    "                index = np.where(df_count_1[\"Source\"] == \"GNPS\")[0]\n",
    "                merged_df.loc[mer, \"SMILES\"] = df_count_1[\"SMILES\"][index[0]]\n",
    "                merged_df.loc[mer, \"AnnotationCount\"] = df_count_1[\"Count\"][index[0]]\n",
    "                comp = pcp.get_compounds(df_count_1[\"SMILES\"][index[0]], 'smiles')\n",
    "                try:\n",
    "                    if comp:\n",
    "                        for c in comp:\n",
    "                            merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                            merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                            merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "                except Exception:\n",
    "                    pass\n",
    "                merged_df[\"superclass\"][mer] = np.nan\n",
    "                merged_df[\"class\"][mer] = np.nan\n",
    "                merged_df[\"subclass\"][mer] = np.nan\n",
    "                merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "\n",
    "                merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "                    str(merged_df[\"AnnotationSources\"][mer]) + \"|GNPS\"\n",
    "                )\n",
    "                \n",
    "            elif \"SIRIUS\" in np.unique(df_count_1[\"Source\"]):\n",
    "                index = np.where(df_count_1[\"Source\"] == \"SIRIUS\")[0]\n",
    "                merged_df.loc[mer, \"SMILES\"] = df_count_1[\"SMILES\"][index[0]]\n",
    "                merged_df.loc[mer, \"AnnotationCount\"] = df_count_1[\"Count\"][index[0]]\n",
    "                merged_df[\"Formula\"] = sirius_df[\"molecularFormula\"][0]\n",
    "                merged_df[\"superclass\"] = sirius_df[\"superclass\"][0]\n",
    "                merged_df[\"class\"] = sirius_df[\"class\"][0]\n",
    "                merged_df[\"subclass\"] = sirius_df[\"subclass\"][0]\n",
    "                merged_df[\"ClassificationSource\"] = \"CANOPUS\"\n",
    "                merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "                    str(merged_df[\"AnnotationSources\"][mer]) + \"|SIRIUS\"\n",
    "                )\n",
    "            elif \"MassBank\" in np.unique(df_count_1[\"Source\"]):\n",
    "                index = np.where(df_count_1[\"Source\"] == \"MassBank\")[0]\n",
    "                merged_df.loc[mer, \"SMILES\"] = df_count_1[\"SMILES\"][index[0]]\n",
    "                merged_df.loc[mer, \"AnnotationCount\"] = df_count_1[\"Count\"][index[0]]\n",
    "                \n",
    "                comp = pcp.get_compounds(df_count_1[\"SMILES\"][index[0]], 'smiles')\n",
    "                try:\n",
    "                    if comp:\n",
    "                        for c in comp:\n",
    "                            merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                            merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                            merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "                except Exception:\n",
    "                    pass\n",
    "                merged_df[\"superclass\"][mer] = np.nan\n",
    "                merged_df[\"class\"][mer] = np.nan\n",
    "                merged_df[\"subclass\"][mer] = np.nan\n",
    "                merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "                merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "                    str(merged_df[\"AnnotationSources\"][mer]) + \"|MassBank\"\n",
    "                )\n",
    "\n",
    "            elif \"HMDB\" in np.unique(df_count_1[\"Source\"]):\n",
    "                index = np.where(df_count_1[\"Source\"] == \"HMDB\")[0]\n",
    "                merged_df.loc[mer, \"SMILES\"] = df_count_1[\"SMILES\"][index[0]]\n",
    "                merged_df.loc[mer, \"AnnotationCount\"] = df_count_1[\"Count\"][index[0]]\n",
    "                \n",
    "                comp = pcp.get_compounds(df_count_1[\"SMILES\"][index[0]], 'smiles')\n",
    "                try:\n",
    "                    if comp:\n",
    "                        for c in comp:\n",
    "                            merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                            merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                            merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "                except Exception:\n",
    "                    pass\n",
    "                merged_df[\"superclass\"][mer] = np.nan\n",
    "                merged_df[\"class\"][mer] = np.nan\n",
    "                merged_df[\"subclass\"][mer] = np.nan\n",
    "                merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "                merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "                    str(merged_df[\"AnnotationSources\"][mer]) + \"|HMDB\"\n",
    "                )\n",
    "\n",
    "            \n",
    "            merged_df[\"AnnotationSources\"][mer] = merged_df[\"AnnotationSources\"][\n",
    "                mer\n",
    "            ].replace(\"nan|\", \"\")\n",
    "            if \"SIRIUS\" == merged_df[\"AnnotationSources\"][mer]:\n",
    "                merged_df.loc[mer, \"MSILevel\"] = 3\n",
    "\n",
    "            if (\n",
    "                \"HMDB\" in merged_df[\"AnnotationSources\"][mer]\n",
    "                or \"GNPS\" in merged_df[\"AnnotationSources\"][mer]\n",
    "                or \"MassBank\" in merged_df[\"AnnotationSources\"][mer]\n",
    "            ):\n",
    "                merged_df.loc[mer, \"MSILevel\"] = 2\n",
    "                merged_df[\"superclass\"][mer] = np.nan\n",
    "                merged_df[\"class\"][mer] = np.nan\n",
    "                merged_df[\"subclass\"][mer] = np.nan\n",
    "                merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "            \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7d8b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sources_2(candidates_with_counts, merged_df, mer, sirius_df):\n",
    "\n",
    "    \"\"\"if only 2 sources have confirmed the presence of a certain SMILES.\n",
    "    This holds true when each candidate SMILES has only two sources. The\n",
    "    function selects the best candidate and adds the two sources as\n",
    "    annotation sources\n",
    "\n",
    "    Parameters:\n",
    "    candidates_with_counts: this is the result from the function add_count_column\n",
    "    and contains a ordered dataframe, with the most sourced SMILES at top.\n",
    "    merged_df: dataframe that contains all features from the input mzML file\n",
    "\n",
    "    Returns:\n",
    "    merged_df: with added top SMILES, Annotation Sources, Annotation Count, and\n",
    "    MSI-Level\n",
    "\n",
    "    Usage:\n",
    "    sources_2(candidates_with_counts, merged_df, mer)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df_count_2 = candidates_with_counts[candidates_with_counts[\"Count\"] == 2]\n",
    "    df_count_2[\"rank_num\"] = [counts.split(\"_\")[1] for counts in df_count_2[\"ranks\"]]\n",
    "    df_count_2[\"rank_num\"] = [int(x) for x in df_count_2[\"rank_num\"]]\n",
    "    df_count_2 = df_count_2.sort_values(by=\"rank_num\")\n",
    "    df_countnew = df_count_2[df_count_2[\"rank_num\"] == min(df_count_2[\"rank_num\"])]\n",
    "    df_countnew[\"count_min\"] = [\n",
    "        str(df_countnew[\"SIRIUS\"][x])\n",
    "        + str(df_countnew[\"GNPS\"][x])\n",
    "        + str(df_countnew[\"MassBank\"][x])\n",
    "        + str(df_countnew[\"HMDB\"][x])\n",
    "        for x, row in df_countnew.iterrows()\n",
    "    ]\n",
    "    df_countnew[\"count_max\"] = [x.count(\"_1\") for x in df_countnew[\"count_min\"]]\n",
    "    df_countnew = df_countnew.sort_values(by=\"count_max\", ascending=False)\n",
    "\n",
    "    df_countnew.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    gnps_indices = list(df_countnew[(df_countnew[\"GNPS\"].notnull())].index)\n",
    "    mbank_indices = list(df_countnew[(df_countnew[\"MassBank\"].notnull())].index)\n",
    "    hmdb_indices = list(df_countnew[(df_countnew[\"HMDB\"].notnull())].index)\n",
    "    sirius_indices = list(df_countnew[(df_countnew[\"SIRIUS\"].notnull())].index)\n",
    "\n",
    "    if 0 in sirius_indices:\n",
    "        # print(\"sirius\")\n",
    "        merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "            str(merged_df[\"AnnotationSources\"][mer]) + \"|SIRIUS\"\n",
    "        )\n",
    "    if 0 in mbank_indices:\n",
    "        merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "            str(merged_df[\"AnnotationSources\"][mer]) + \"|MassBank\"\n",
    "        )\n",
    "        # print(\"mbank\")\n",
    "    if 0 in hmdb_indices:\n",
    "        merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "            str(merged_df[\"AnnotationSources\"][mer]) + \"|HMDB\"\n",
    "        )\n",
    "        # print(\"hmdb\")\n",
    "    if 0 in gnps_indices:\n",
    "        merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "            str(merged_df[\"AnnotationSources\"][mer]) + \"|GNPS\"\n",
    "        )\n",
    "        # print(\"gnps\")\n",
    "    if (\n",
    "        \"HMDB\" in merged_df[\"AnnotationSources\"][mer]\n",
    "        or \"GNPS\" in merged_df[\"AnnotationSources\"][mer]\n",
    "        or \"MassBank\" in merged_df[\"AnnotationSources\"][mer]\n",
    "    ):\n",
    "        merged_df.loc[mer, \"MSILevel\"] = 2\n",
    "    if \"nan|SIRIUS\" == merged_df[\"AnnotationSources\"][mer]:\n",
    "        merged_df.loc[mer, \"MSILevel\"] = 3\n",
    "        \n",
    "\n",
    "    merged_df[\"AnnotationSources\"][mer] = merged_df[\"AnnotationSources\"][mer].replace(\n",
    "        \"nan|\", \"\"\n",
    "    )\n",
    "    merged_df.loc[mer, \"AnnotationCount\"] = df_countnew[\"Count\"][0]\n",
    "\n",
    "    if \"GNPS\" in merged_df[\"AnnotationSources\"][mer]:\n",
    "        if len(df_count_2.loc[df_count_2[\"Source\"] == \"GNPS\"]) == 1:\n",
    "            new = df_count_2.loc[df_count_2[\"Source\"] == \"GNPS\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            comp = pcp.get_compounds(new[\"SMILES\"][0], 'smiles')\n",
    "            try:\n",
    "                if comp:\n",
    "                    for c in comp:\n",
    "                        merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                        merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                        merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "            except Exception:\n",
    "                pass\n",
    "            merged_df[\"superclass\"][mer] = np.nan\n",
    "            merged_df[\"class\"][mer] = np.nan\n",
    "            merged_df[\"subclass\"][mer] = np.nan\n",
    "            merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "        elif len(df_count_2.loc[df_count_2[\"Source\"] == \"GNPS\"]) > 1:\n",
    "            new = df_count_2.loc[df_count_2[\"Source\"] == \"GNPS\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            comp = pcp.get_compounds(new[\"SMILES\"][0], 'smiles')\n",
    "            try:\n",
    "                if comp:\n",
    "                    for c in comp:\n",
    "                        merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                        merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                        merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "            except Exception:\n",
    "                pass\n",
    "            merged_df[\"superclass\"][mer] = np.nan\n",
    "            merged_df[\"class\"][mer] = np.nan\n",
    "            merged_df[\"subclass\"][mer] = np.nan\n",
    "            merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        \n",
    "    elif \"SIRIUS\" in merged_df[\"AnnotationSources\"][mer]:\n",
    "        if len(df_count_2.loc[df_count_2[\"Source\"] == \"SIRIUS\"]) == 1:\n",
    "            new = df_count_2.loc[df_count_2[\"Source\"] == \"SIRIUS\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            merged_df[\"Formula\"] = sirius_df[\"molecularFormula\"][0]\n",
    "            merged_df[\"superclass\"] = sirius_df[\"superclass\"][0]\n",
    "            merged_df[\"class\"] = sirius_df[\"class\"][0]\n",
    "            merged_df[\"subclass\"] = sirius_df[\"subclass\"][0]\n",
    "            merged_df[\"ClassificationSource\"] = \"CANOPUS\"\n",
    "        elif len(df_count_2.loc[df_count_2[\"Source\"] == \"SIRIUS\"]) > 1:\n",
    "            new = df_count_2.loc[df_count_2[\"Source\"] == \"SIRIUS\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            merged_df[\"Formula\"] = sirius_df[\"molecularFormula\"][0]\n",
    "            merged_df[\"superclass\"] = sirius_df[\"superclass\"][0]\n",
    "            merged_df[\"class\"] = sirius_df[\"class\"][0]\n",
    "            merged_df[\"subclass\"] = sirius_df[\"subclass\"][0]\n",
    "            merged_df[\"ClassificationSource\"] = \"CANOPUS\"\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "    elif \"MassBank\" in merged_df[\"AnnotationSources\"][mer]:\n",
    "        if len(df_count_2.loc[df_count_2[\"Source\"] == \"MassBank\"]) == 1:\n",
    "            new = df_count_2.loc[df_count_2[\"Source\"] == \"MassBank\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            comp = pcp.get_compounds(merged_df[\"SMILES\"][mer], 'smiles')\n",
    "            try:\n",
    "                if comp:\n",
    "                    for c in comp:\n",
    "                        merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                        merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                        merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "            except Exception:\n",
    "                pass\n",
    "            merged_df[\"superclass\"][mer] = np.nan\n",
    "            merged_df[\"class\"][mer] = np.nan\n",
    "            merged_df[\"subclass\"][mer] = np.nan\n",
    "            merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "        elif len(df_count_2.loc[df_count_2[\"Source\"] == \"MassBank\"]) > 1:\n",
    "            new = df_count_2.loc[df_count_2[\"Source\"] == \"MassBank\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            comp = pcp.get_compounds(new[\"SMILES\"][0], 'smiles')\n",
    "            try:\n",
    "                if comp:\n",
    "                    for c in comp:\n",
    "                        merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                        merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                        merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "            except Exception:\n",
    "                pass\n",
    "            merged_df[\"superclass\"][mer] = np.nan\n",
    "            merged_df[\"class\"][mer] = np.nan\n",
    "            merged_df[\"subclass\"][mer] = np.nan\n",
    "            merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "        else:\n",
    "            pass\n",
    "    elif \"HMDB\" in merged_df[\"AnnotationSources\"][mer]:\n",
    "        if len(df_count_2.loc[df_count_2[\"Source\"] == \"HMDB\"]) == 1:\n",
    "            new = df_count_2.loc[df_count_2[\"Source\"] == \"HMDB\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            comp = pcp.get_compounds(new[\"SMILES\"][0], 'smiles')\n",
    "            try:\n",
    "                if comp:\n",
    "                    for c in comp:\n",
    "                        merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                        merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                        merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "            except Exception:\n",
    "                pass\n",
    "            merged_df[\"superclass\"] = np.nan\n",
    "            merged_df[\"class\"] = np.nan\n",
    "            merged_df[\"subclass\"] = np.nan\n",
    "            merged_df[\"ClassificationSource\"] = np.nan\n",
    "        elif len(df_count_2.loc[df_count_2[\"Source\"] == \"HMDB\"]) > 1:\n",
    "            new = df_count_2.loc[df_count_2[\"Source\"] == \"HMDB\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            comp = pcp.get_compounds(new[\"SMILES\"][0], 'smiles')\n",
    "            try:\n",
    "                if comp:\n",
    "                    for c in comp:\n",
    "                        merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                        merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                        merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "            except Exception:\n",
    "                pass\n",
    "            merged_df[\"superclass\"][mer] = np.nan\n",
    "            merged_df[\"class\"][mer] = np.nan\n",
    "            merged_df[\"subclass\"][mer] = np.nan\n",
    "            merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f71836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sources_3(candidates_with_counts, merged_df, mer, sirius_df):\n",
    "\n",
    "    \"\"\"if only 3 sources have confirmed the presence of a certain SMILES.\n",
    "    This holds true when each candidate SMILES has only 3 sources. The\n",
    "    function selects the best candidate and adds the 3 sources as\n",
    "    annotation sources\n",
    "\n",
    "    Parameters:\n",
    "    candidates_with_counts: this is the result from the function add_count_column\n",
    "    and contains a ordered dataframe, with the most sourced SMILES at top.\n",
    "    merged_df: dataframe that contains all features from the input mzML file\n",
    "\n",
    "    Returns:\n",
    "    merged_df: with added top SMILES, Annotation Sources, Annotation Count, and\n",
    "    MSI-Level\n",
    "\n",
    "    Usage:\n",
    "    sources_2(candidates_with_counts, merged_df, mer)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df_count_3 = candidates_with_counts[candidates_with_counts[\"Count\"] == 3]\n",
    "    df_count_3[\"rank_num\"] = [counts.split(\"_\")[1] for counts in df_count_3[\"ranks\"]]\n",
    "    df_count_3[\"rank_num\"] = [int(x) for x in df_count_3[\"rank_num\"]]\n",
    "\n",
    "    df_count_3 = df_count_3.sort_values(by=\"rank_num\")\n",
    "    df_count_3 = df_count_3[df_count_3[\"rank_num\"] == min(df_count_3[\"rank_num\"])]\n",
    "    df_count_3[\"count_min\"] = [\n",
    "        str(df_count_3[\"SIRIUS\"][x])\n",
    "        + str(df_count_3[\"GNPS\"][x])\n",
    "        + str(df_count_3[\"MassBank\"][x])\n",
    "        + str(df_count_3[\"HMDB\"][x])\n",
    "        for x, row in df_count_3.iterrows()\n",
    "    ]\n",
    "    df_count_3[\"count_max\"] = [x.count(\"_1\") for x in df_count_3[\"count_min\"]]\n",
    "    df_count_3 = df_count_3.sort_values(by=\"count_max\", ascending=False)\n",
    "\n",
    "    df_count_3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    merged_df.loc[mer, \"AnnotationCount\"] = df_count_3[\"Count\"][0]\n",
    "\n",
    "    gnps_indices = list(df_count_3[(df_count_3[\"GNPS\"].notnull())].index)\n",
    "    mbank_indices = list(df_count_3[(df_count_3[\"MassBank\"].notnull())].index)\n",
    "    hmdb_indices = list(df_count_3[(df_count_3[\"HMDB\"].notnull())].index)\n",
    "    sirius_indices = list(df_count_3[(df_count_3[\"SIRIUS\"].notnull())].index)\n",
    "\n",
    "    if 0 in sirius_indices:\n",
    "        # print(\"sirius\")\n",
    "        merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "            str(merged_df[\"AnnotationSources\"][mer]) + \"|SIRIUS\"\n",
    "        )\n",
    "    if 0 in mbank_indices:\n",
    "        merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "            str(merged_df[\"AnnotationSources\"][mer]) + \"|MassBank\"\n",
    "        )\n",
    "        # print(\"mbank\")\n",
    "    if 0 in hmdb_indices:\n",
    "        merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "            str(merged_df[\"AnnotationSources\"][mer]) + \"|HMDB\"\n",
    "        )\n",
    "        # print(\"hmdb\")\n",
    "    if 0 in gnps_indices:\n",
    "        merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "            str(merged_df[\"AnnotationSources\"][mer]) + \"|GNPS\"\n",
    "        )\n",
    "        # print(\"gnps\")\n",
    "    if \"nan|SIRIUS\" == merged_df[\"AnnotationSources\"][mer]:\n",
    "        merged_df.loc[mer, \"MSILevel\"] = 3\n",
    "\n",
    "    merged_df[\"AnnotationSources\"][mer] = merged_df[\"AnnotationSources\"][mer].replace(\n",
    "        \"nan|\", \"\"\n",
    "    )\n",
    "    if (\n",
    "        \"HMDB\" in merged_df[\"AnnotationSources\"][mer]\n",
    "        or \"GNPS\" in merged_df[\"AnnotationSources\"][mer]\n",
    "        or \"MassBank\" in merged_df[\"AnnotationSources\"][mer]\n",
    "    ):\n",
    "        merged_df.loc[mer, \"MSILevel\"] = 2\n",
    "    \n",
    "\n",
    "    if \"GNPS\" in merged_df[\"AnnotationSources\"][mer]:\n",
    "        if len(df_count_3.loc[df_count_3[\"Source\"] == \"GNPS\"]) == 1:\n",
    "            new = df_count_3.loc[df_count_3[\"Source\"] == \"GNPS\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            comp = pcp.get_compounds(new[\"SMILES\"][0], 'smiles')\n",
    "            try:\n",
    "                if comp:\n",
    "                    for c in comp:\n",
    "                        merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                        merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                        merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "            except Exception:\n",
    "                pass\n",
    "            merged_df[\"superclass\"][mer] = np.nan\n",
    "            merged_df[\"class\"][mer] = np.nan\n",
    "            merged_df[\"subclass\"][mer] = np.nan\n",
    "            merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "        elif len(df_count_3.loc[df_count_3[\"Source\"] == \"GNPS\"]) > 1:\n",
    "            new = df_count_3.loc[df_count_3[\"Source\"] == \"GNPS\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            comp = pcp.get_compounds(new[\"SMILES\"][0], 'smiles')\n",
    "            try:\n",
    "                if comp:\n",
    "                    for c in comp:\n",
    "                        merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                        merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                        merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "            except Exception:\n",
    "                pass\n",
    "            merged_df[\"superclass\"][mer] = np.nan\n",
    "            merged_df[\"class\"][mer] = np.nan\n",
    "            merged_df[\"subclass\"][mer] = np.nan\n",
    "            merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    elif \"SIRIUS\" in merged_df[\"AnnotationSources\"][mer]:\n",
    "        if len(df_count_3.loc[df_count_3[\"Source\"] == \"SIRIUS\"]) == 1:\n",
    "            new = df_count_3.loc[df_count_3[\"Source\"] == \"SIRIUS\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            merged_df[\"Formula\"] = sirius_df[\"molecularFormula\"][0]\n",
    "            merged_df[\"superclass\"] = sirius_df[\"superclass\"][0]\n",
    "            merged_df[\"class\"] = sirius_df[\"class\"][0]\n",
    "            merged_df[\"subclass\"] = sirius_df[\"subclass\"][0]\n",
    "            merged_df[\"ClassificationSource\"] = \"CANOPUS\"\n",
    "        elif len(df_count_3.loc[df_count_3[\"Source\"] == \"SIRIUS\"]) > 1:\n",
    "            new = df_count_3.loc[df_count_3[\"Source\"] == \"SIRIUS\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            merged_df[\"Formula\"] = sirius_df[\"molecularFormula\"][0]\n",
    "            merged_df[\"superclass\"] = sirius_df[\"superclass\"][0]\n",
    "            merged_df[\"class\"] = sirius_df[\"class\"][0]\n",
    "            merged_df[\"subclass\"] = sirius_df[\"subclass\"][0]\n",
    "            merged_df[\"ClassificationSource\"] = \"CANOPUS\"\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    elif \"MassBank\" in merged_df[\"AnnotationSources\"][mer]:\n",
    "        if len(df_count_3.loc[df_count_3[\"Source\"] == \"MassBank\"]) == 1:\n",
    "            new = df_count_3.loc[df_count_3[\"Source\"] == \"MassBank\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            comp = pcp.get_compounds(new[\"SMILES\"][0], 'smiles')\n",
    "            try:\n",
    "                if comp:\n",
    "                    for c in comp:\n",
    "                        merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                        merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                        merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "            except Exception:\n",
    "                pass\n",
    "            merged_df[\"superclass\"][mer] = np.nan\n",
    "            merged_df[\"class\"][mer] = np.nan\n",
    "            merged_df[\"subclass\"][mer] = np.nan\n",
    "            merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "        elif len(df_count_3.loc[df_count_3[\"Source\"] == \"MassBank\"]) > 1:\n",
    "            new = df_count_3.loc[df_count_3[\"Source\"] == \"MassBank\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            comp = pcp.get_compounds(new[\"SMILES\"][0], 'smiles')\n",
    "            try:\n",
    "                if comp:\n",
    "                    for c in comp:\n",
    "                        merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                        merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                        merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "            except Exception:\n",
    "                pass\n",
    "            merged_df[\"superclass\"][mer] = np.nan\n",
    "            merged_df[\"class\"][mer] = np.nan\n",
    "            merged_df[\"subclass\"][mer] = np.nan\n",
    "            merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "        else:\n",
    "            pass\n",
    "    elif \"HMDB\" in merged_df[\"AnnotationSources\"][mer]:\n",
    "        if len(df_count_3.loc[df_count_3[\"Source\"] == \"HMDB\"]) == 1:\n",
    "            new = df_count_3.loc[df_count_3[\"Source\"] == \"HMDB\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            comp = pcp.get_compounds(new[\"SMILES\"][0], 'smiles')\n",
    "            try:\n",
    "                if comp:\n",
    "                    for c in comp:\n",
    "                        merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                        merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                        merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "            except Exception:\n",
    "                pass\n",
    "            merged_df[\"superclass\"][mer] = np.nan\n",
    "            merged_df[\"class\"][mer] = np.nan\n",
    "            merged_df[\"subclass\"][mer] = np.nan\n",
    "            merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "        elif len(df_count_3.loc[df_count_3[\"Source\"] == \"HMDB\"]) > 1:\n",
    "            new = df_count_3.loc[df_count_3[\"Source\"] == \"HMDB\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            comp = pcp.get_compounds(new[\"SMILES\"][0], 'smiles')\n",
    "            try:\n",
    "                if comp:\n",
    "                    for c in comp:\n",
    "                        merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                        merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                        merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "            except Exception:\n",
    "                pass\n",
    "            merged_df[\"superclass\"][mer] = np.nan\n",
    "            merged_df[\"class\"][mer] = np.nan\n",
    "            merged_df[\"subclass\"][mer] = np.nan\n",
    "            merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    heavy_atoms = [\"C\", \"N\", \"P\", \"O\", \"S\"]\n",
    "\n",
    "    Mol = []\n",
    "\n",
    "    for j in list(df_count_3[\"SMILES\"]):\n",
    "        if not isNaN(j):\n",
    "            # print(type(j))\n",
    "            mol2 = Chem.MolFromSmiles(j)\n",
    "            Mol.append(mol2)\n",
    "\n",
    "    if len(Mol) >= 2:\n",
    "        res = rdFMCS.FindMCS(Mol)\n",
    "        sm_res = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "        # if there are atleast 3 heavy atoms in the MCSS, then add it to the result file\n",
    "        elem = [ele for ele in heavy_atoms if (ele in sm_res)]\n",
    "        if elem and len(sm_res) >= 3:\n",
    "            merged_df.loc[mer, \"MCSS\"] = Chem.MolToSmiles(\n",
    "                Chem.MolFromSmarts(res.smartsString)\n",
    "            )\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a742d789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sources_4(candidates_with_counts, merged_df, mer, sirius_df):\n",
    "\n",
    "    \"\"\"if only 3 sources have confirmed the presence of a certain SMILES.\n",
    "    This holds true when each candidate SMILES has only 3 sources. The\n",
    "    function selects the best candidate and adds the 3 sources as\n",
    "    annotation sources\n",
    "\n",
    "    Parameters:\n",
    "    candidates_with_counts: this is the result from the function add_count_column\n",
    "    and contains a ordered dataframe, with the most sourced SMILES at top.\n",
    "    merged_df: dataframe that contains all features from the input mzML file\n",
    "\n",
    "    Returns:\n",
    "    merged_df: with added top SMILES, Annotation Sources, Annotation Count, and\n",
    "    MSI-Level\n",
    "\n",
    "    Usage:\n",
    "    sources_2(candidates_with_counts, merged_df, mer)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df_count_4 = candidates_with_counts[candidates_with_counts[\"Count\"] == 4]\n",
    "    df_count_4[\"rank_num\"] = [counts.split(\"_\")[1] for counts in df_count_4[\"ranks\"]]\n",
    "    df_count_4[\"rank_num\"] = [int(x) for x in df_count_4[\"rank_num\"]]\n",
    "    df_count_4 = df_count_4.sort_values(by=\"rank_num\")\n",
    "    df_count_4 = df_count_4[df_count_4[\"rank_num\"] == min(df_count_4[\"rank_num\"])]\n",
    "    df_count_4[\"count_min\"] = [\n",
    "        str(df_count_4[\"SIRIUS\"][x])\n",
    "        + str(df_count_4[\"GNPS\"][x])\n",
    "        + str(df_count_4[\"MassBank\"][x])\n",
    "        + str(df_count_4[\"HMDB\"][x])\n",
    "        for x, row in df_count_4.iterrows()\n",
    "    ]\n",
    "    df_count_4[\"count_max\"] = [x.count(\"_1\") for x in df_count_4[\"count_min\"]]\n",
    "    df_count_4 = df_count_4.sort_values(by=\"count_max\", ascending=False)\n",
    "\n",
    "    df_count_4.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    merged_df.loc[mer, \"AnnotationCount\"] = df_count_4[\"Count\"][0]\n",
    "\n",
    "    gnps_indices = list(df_count_4[(df_count_4[\"GNPS\"].notnull())].index)\n",
    "    mbank_indices = list(df_count_4[(df_count_4[\"MassBank\"].notnull())].index)\n",
    "    hmdb_indices = list(df_count_4[(df_count_4[\"HMDB\"].notnull())].index)\n",
    "    sirius_indices = list(df_count_4[(df_count_4[\"SIRIUS\"].notnull())].index)\n",
    "\n",
    "    if 0 in sirius_indices:\n",
    "        # print(\"sirius\")\n",
    "        merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "            str(merged_df[\"AnnotationSources\"][mer]) + \"|SIRIUS\"\n",
    "        )\n",
    "    if 0 in mbank_indices:\n",
    "        merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "            str(merged_df[\"AnnotationSources\"][mer]) + \"|Massbank\"\n",
    "        )\n",
    "        # print(\"mbank\")\n",
    "    if 0 in hmdb_indices:\n",
    "        merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "            str(merged_df[\"AnnotationSources\"][mer]) + \"|HMDB\"\n",
    "        )\n",
    "        # print(\"hmdb\")\n",
    "    if 0 in gnps_indices:\n",
    "        merged_df.loc[mer, \"AnnotationSources\"] = (\n",
    "            str(merged_df[\"AnnotationSources\"][mer]) + \"|GNPS\"\n",
    "        )\n",
    "        # print(\"gnps\")\n",
    "    \n",
    "    if \"nan|SIRIUS\" == merged_df[\"AnnotationSources\"][mer]:\n",
    "        merged_df.loc[mer, \"MSILevel\"] = 3\n",
    "\n",
    "    merged_df[\"AnnotationSources\"][mer] = merged_df[\"AnnotationSources\"][mer].replace(\n",
    "        \"nan|\", \"\"\n",
    "    )\n",
    "    \n",
    "    if (\n",
    "        \"HMDB\" in merged_df[\"AnnotationSources\"][mer]\n",
    "        or \"GNPS\" in merged_df[\"AnnotationSources\"][mer]\n",
    "        or \"MassBank\" in merged_df[\"AnnotationSources\"][mer]\n",
    "    ):\n",
    "        merged_df.loc[mer, \"MSILevel\"] = 2\n",
    "    \n",
    "\n",
    "    \n",
    "    if \"GNPS\" in merged_df[\"AnnotationSources\"][mer]:\n",
    "        if len(df_count_4.loc[df_count_4[\"Source\"] == \"GNPS\"]) == 1:\n",
    "            new = df_count_4.loc[df_count_4[\"Source\"] == \"GNPS\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            comp = pcp.get_compounds(new[\"SMILES\"][0], 'smiles')\n",
    "            try:\n",
    "                if comp:\n",
    "                    for c in comp:\n",
    "                        merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                        merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                        merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "            except Exception:\n",
    "                pass\n",
    "            merged_df[\"superclass\"][mer] = np.nan\n",
    "            merged_df[\"class\"][mer] = np.nan\n",
    "            merged_df[\"subclass\"][mer] = np.nan\n",
    "            merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "        elif len(df_count_4.loc[df_count_4[\"Source\"] == \"GNPS\"]) > 1:\n",
    "            new = df_count_4.loc[df_count_4[\"Source\"] == \"GNPS\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            comp = pcp.get_compounds(new[\"SMILES\"][0], 'smiles')\n",
    "            try:\n",
    "                if comp:\n",
    "                    for c in comp:\n",
    "                        merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                        merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                        merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "            except Exception:\n",
    "                pass\n",
    "            merged_df[\"superclass\"][mer] = np.nan\n",
    "            merged_df[\"class\"][mer] = np.nan\n",
    "            merged_df[\"subclass\"][mer] = np.nan\n",
    "            merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    elif \"SIRIUS\" in merged_df[\"AnnotationSources\"][mer]:\n",
    "        if len(df_count_4.loc[df_count_4[\"Source\"] == \"SIRIUS\"]) == 1:\n",
    "            new = df_count_4.loc[df_count_4[\"Source\"] == \"SIRIUS\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            merged_df[\"Formula\"] = sirius_df[\"molecularFormula\"][0]\n",
    "            merged_df[\"superclass\"] = sirius_df[\"superclass\"][0]\n",
    "            merged_df[\"class\"] = sirius_df[\"class\"][0]\n",
    "            merged_df[\"subclass\"] = sirius_df[\"subclass\"][0]\n",
    "            merged_df[\"ClassificationSource\"] = \"CANOPUS\"\n",
    "        elif len(df_count_4.loc[df_count_4[\"Source\"] == \"SIRIUS\"]) > 1:\n",
    "            new = df_count_4.loc[df_count_4[\"Source\"] == \"SIRIUS\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            merged_df[\"Formula\"] = sirius_df[\"molecularFormula\"][0]\n",
    "            merged_df[\"superclass\"] = sirius_df[\"superclass\"][0]\n",
    "            merged_df[\"class\"] = sirius_df[\"class\"][0]\n",
    "            merged_df[\"subclass\"] = sirius_df[\"subclass\"][0]\n",
    "            merged_df[\"ClassificationSource\"] = \"CANOPUS\"\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    elif \"MassBank\" in merged_df[\"AnnotationSources\"][mer]:\n",
    "        if len(df_count_4.loc[df_count_4[\"Source\"] == \"MassBank\"]) == 1:\n",
    "            new = df_count_4.loc[df_count_4[\"Source\"] == \"MassBank\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            comp = pcp.get_compounds(new[\"SMILES\"][0], 'smiles')\n",
    "            try:\n",
    "                if comp:\n",
    "                    for c in comp:\n",
    "                        merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                        merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                        merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "            except Exception:\n",
    "                pass\n",
    "            merged_df[\"superclass\"][mer] = np.nan\n",
    "            merged_df[\"class\"][mer] = np.nan\n",
    "            merged_df[\"subclass\"][mer] = np.nan\n",
    "            merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "        elif len(df_count_4.loc[df_count_4[\"Source\"] == \"MassBank\"]) > 1:\n",
    "            new = df_count_4.loc[df_count_4[\"Source\"] == \"MassBank\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            comp = pcp.get_compounds(new[\"SMILES\"][0], 'smiles')\n",
    "            try:\n",
    "                if comp:\n",
    "                    for c in comp:\n",
    "                        merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                        merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                        merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "            except Exception:\n",
    "                pass\n",
    "            merged_df[\"superclass\"][mer] = np.nan\n",
    "            merged_df[\"class\"][mer] = np.nan\n",
    "            merged_df[\"subclass\"][mer] = np.nan\n",
    "            merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "        else:\n",
    "            pass\n",
    "    elif \"HMDB\" in merged_df[\"AnnotationSources\"][mer]:\n",
    "        if len(df_count_4.loc[df_count_4[\"Source\"] == \"HMDB\"]) == 1:\n",
    "            new = df_count_4.loc[df_count_4[\"Source\"] == \"HMDB\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            comp = pcp.get_compounds(new[\"SMILES\"][0], 'smiles')\n",
    "            try:\n",
    "                if comp:\n",
    "                    for c in comp:\n",
    "                        merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                        merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                        merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "            except Exception:\n",
    "                pass\n",
    "            merged_df[\"superclass\"][mer] = np.nan\n",
    "            merged_df[\"class\"][mer] = np.nan\n",
    "            merged_df[\"subclass\"][mer] = np.nan\n",
    "            merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "        elif len(df_count_4.loc[df_count_4[\"Source\"] == \"HMDB\"]) > 1:\n",
    "            new = df_count_4.loc[df_count_4[\"Source\"] == \"HMDB\"]\n",
    "            new.reset_index(drop=True, inplace=True)\n",
    "            merged_df.loc[mer, \"SMILES\"] = new[\"SMILES\"][0]\n",
    "            comp = pcp.get_compounds(new[\"SMILES\"][0], 'smiles')\n",
    "            try:\n",
    "                if comp:\n",
    "                    for c in comp:\n",
    "                        merged_df[\"synonyms\"][mer] = c.synonyms\n",
    "                        merged_df.loc[mer, \"IUPAC\"] = c.iupac_name\n",
    "                        merged_df.loc[mer, \"Formula\"] = c.molecular_formula\n",
    "            except Exception:\n",
    "                pass\n",
    "            merged_df[\"superclass\"][mer] = np.nan\n",
    "            merged_df[\"class\"][mer] = np.nan\n",
    "            merged_df[\"subclass\"][mer] = np.nan\n",
    "            merged_df[\"ClassificationSource\"][mer] = np.nan\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    heavy_atoms = [\"C\", \"N\", \"P\", \"O\", \"S\"]\n",
    "\n",
    "    Mol = []\n",
    "\n",
    "    for j in list(df_count_4[\"SMILES\"]):\n",
    "        if not isNaN(j):\n",
    "            # print(type(j))\n",
    "            mol2 = Chem.MolFromSmiles(j)\n",
    "            Mol.append(mol2)\n",
    "\n",
    "    if len(Mol) >= 2:\n",
    "        res = rdFMCS.FindMCS(Mol)\n",
    "        sm_res = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "        # if there are atleast 3 heavy atoms in the MCSS, then add it to the result file\n",
    "        elem = [ele for ele in heavy_atoms if (ele in sm_res)]\n",
    "        if elem and len(sm_res) >= 3:\n",
    "            merged_df.loc[mer, \"MCSS\"] = Chem.MolToSmiles(\n",
    "                Chem.MolFromSmarts(res.smartsString)\n",
    "            )\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623cfd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkSMILES_validity(input_dir, resultcsv):\n",
    "    \n",
    "\n",
    "    \"\"\"checkSMILES_validity does exactly as the name says, using\n",
    "    RDKit, whether the SMILES are invalid or have invalid\n",
    "    chemistry\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML\n",
    "    files and their respective result directories are stored.\n",
    "\n",
    "    results: df from combine_CuratedR\n",
    "\n",
    "    Returns:\n",
    "    dataframe: with valid SMILES\n",
    "    csv: \"MetabolomicsResults/final_curation_with_validSMILES.csv\"\n",
    "\n",
    "    Usage:\n",
    "    checkSMILES_validity(input_dir = \"usr/project/\", results)\n",
    "\n",
    "    \"\"\"\n",
    "    results = pd.read_csv(resultcsv)\n",
    "    # check validity of SMILES\n",
    "    for i, row in results.iterrows():\n",
    "        if not isNaN(results[\"SMILES\"][i]):\n",
    "            m = Chem.MolFromSmiles(results[\"SMILES\"][i], sanitize=False)\n",
    "            if m is None:\n",
    "                results[\"SMILES\"][i] = \"invalid_SMILES\"\n",
    "            else:\n",
    "                try:\n",
    "                    Chem.SanitizeMol(m)\n",
    "                except Exception:\n",
    "                    results[\"SMILES\"][i] = \"invalid_chemistry\"\n",
    "#     results.to_csv(\n",
    "#         input_dir + \"MetabolomicsResults/final_curation_with_validSMILES.csv\"\n",
    "#     )\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f4fcd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CandidateSelection_SimilarityandIdentity(input_dir, standards = False):\n",
    "\n",
    "    # entry is all files and folders in input_dir\n",
    "    for entry in os.listdir(input_dir):\n",
    "        # if the entry is also a directory\n",
    "        if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "            print(entry)\n",
    "\n",
    "            # reach spectra_dereplication folder\n",
    "            sub_dir_spec = input_dir + \"/\" + entry + \"/spectral_dereplication/\"\n",
    "            # reach SIRIUS results\n",
    "            sub_dir_sir = input_dir + \"/\" + entry + \"/insilico/SIRIUS/\"\n",
    "            # new line\n",
    "            if os.path.exists(sub_dir_spec) and os.path.exists(sub_dir_sir):\n",
    "\n",
    "                # list of all csv files in the spectral dereplication foler\n",
    "                spec_msp_csv = glob.glob(\n",
    "                    input_dir + \"/\" + entry + \"/spectral_dereplication\" + \"/*.csv\"\n",
    "                )\n",
    "                # Sirius csv result file\n",
    "                sir_msp_csv = input_dir + \"/\" + entry + \"/insilico/MS1DATA.csv\"\n",
    "\n",
    "                # if both exist; which should be the case, even in case of 0 results\n",
    "                if os.path.exists(sir_msp_csv) and os.path.exists(spec_msp_csv[0]):\n",
    "\n",
    "                    # read both csv files\n",
    "                    spec_msv = pd.read_csv(spec_msp_csv[0])\n",
    "                    sir_msv = pd.read_csv(sir_msp_csv)\n",
    "\n",
    "                    spec_msv = spec_msv[\n",
    "                        [\n",
    "                            \"id_X\",\n",
    "                            \"premz\",\n",
    "                            \"rtmin\",\n",
    "                            \"rtmax\",\n",
    "                            \"rtmed\",\n",
    "                            \"rtmean\",\n",
    "                            \"col_eng\",\n",
    "                            \"pol\",\n",
    "                            \"int\",\n",
    "                            \"source_file\",\n",
    "                            \"mbank_results_csv\",\n",
    "                        ]\n",
    "                    ]\n",
    "                    sir_msv = sir_msv[\n",
    "                        [\n",
    "                            \"id_X\",\n",
    "                            \"premz\",\n",
    "                            \"rtmed\",\n",
    "                            \"rtmean\",\n",
    "                            \"int\",\n",
    "                            \"col_eng\",\n",
    "                            \"pol\",\n",
    "                            \"ms2Peaks\",\n",
    "                            \"ms1Peaks\",\n",
    "                            \"sirius_result_dir\",\n",
    "                        ]\n",
    "                    ]\n",
    "\n",
    "                    merged_df = sir_msv.merge(\n",
    "                        spec_msv,\n",
    "                        how=\"inner\",\n",
    "                        left_on=[\"premz\", \"rtmed\", \"rtmean\", \"int\", \"col_eng\", \"pol\"],\n",
    "                        right_on=[\"premz\", \"rtmed\", \"rtmean\", \"int\", \"col_eng\", \"pol\"],\n",
    "                    )\n",
    "                    merged_df[\"Formula\"] = np.nan\n",
    "                    merged_df[\"SMILES\"] = np.nan\n",
    "                    merged_df[\"IUPAC\"] = np.nan\n",
    "                    merged_df[\"synonyms\"] = np.nan\n",
    "                    merged_df[\"AnnotationSources\"] = np.nan\n",
    "                    merged_df[\"AnnotationCount\"] = np.nan\n",
    "                    merged_df[\"MSILevel\"] = np.nan\n",
    "                    merged_df[\"MCSS\"] = np.nan\n",
    "                    merged_df[\"superclass\"] = np.nan\n",
    "                    merged_df[\"class\"] = np.nan\n",
    "                    merged_df[\"subclass\"] = np.nan\n",
    "                    merged_df[\"ClassificationSource\"] = np.nan\n",
    "\n",
    "                    for mer, rows in merged_df.iterrows():\n",
    "                        print(mer)\n",
    "\n",
    "                        sirius_csv = merged_df[\"sirius_result_dir\"][mer].replace(\n",
    "                            \"./\", input_dir + \"/\"\n",
    "                        )\n",
    "                        print(sirius_csv)\n",
    "                        mbank_csv = merged_df[\"mbank_results_csv\"][mer].replace(\n",
    "                            \"./\", input_dir + \"/\"\n",
    "                        )\n",
    "                        gnps_csv = (\n",
    "                            merged_df[\"mbank_results_csv\"][mer]\n",
    "                            .replace(\"./\", input_dir + \"/\")\n",
    "                            .replace(\"mbank\", \"gnps\")\n",
    "                            .replace(\"MassBank\", \"GNPS\")\n",
    "                        )\n",
    "                        hmdb_csv = (\n",
    "                            merged_df[\"mbank_results_csv\"][mer]\n",
    "                            .replace(\"./\", input_dir + \"/\")\n",
    "                            .replace(\"mbank\", \"hmdb\")\n",
    "                            .replace(\"MassBank\", \"HMDB\")\n",
    "                        )\n",
    "\n",
    "                        if (\n",
    "                            os.path.exists(sirius_csv)\n",
    "                            and os.path.exists(gnps_csv)\n",
    "                            and os.path.exists(mbank_csv)\n",
    "                            and os.path.exists(hmdb_csv)\n",
    "                        ):\n",
    "\n",
    "                            sirius_df = pd.read_csv(sirius_csv)\n",
    "                            if len(sirius_df) > 0:\n",
    "                                \n",
    "                                if \"smiles\" in sirius_df.columns:\n",
    "                                    sirius_df = sirius_df.drop_duplicates(\"smiles\")\n",
    "                                    sirius_df = sirius_df.dropna(subset=[\"smiles\"])\n",
    "                                else:\n",
    "                                    merged_df[\"Formula\"] = sirius_df[\"molecularFormula\"][0]\n",
    "                                    merged_df[\"superclass\"] = sirius_df[\"superclass\"][0]\n",
    "                                    merged_df[\"class\"] = sirius_df[\"class\"][0]\n",
    "                                    merged_df[\"subclass\"] = sirius_df[\"subclass\"][0]\n",
    "                                    merged_df[\"ClassificationSource\"] = \"CANOPUS\"\n",
    "                                \n",
    "\n",
    "                            mbank_df = pd.read_csv(mbank_csv)\n",
    "                            if len(mbank_df) > 0:\n",
    "                                mbank_df = mbank_df.drop_duplicates(\"MBSMILES\")\n",
    "                                mbank_df = mbank_df.dropna(subset=[\"MBSMILES\"])\n",
    "\n",
    "                            gnps_df = pd.read_csv(gnps_csv)\n",
    "                            if len(gnps_df) > 0:\n",
    "                                gnps_df = gnps_df.drop_duplicates(\"GNPSSMILES\")\n",
    "                                gnps_df = gnps_df.dropna(subset=[\"GNPSSMILES\"])\n",
    "\n",
    "                            hmdb_df = pd.read_csv(hmdb_csv)\n",
    "                            if len(hmdb_df) > 0:\n",
    "                                hmdb_df = hmdb_df.drop_duplicates(\"HMDBSMILES\")\n",
    "                                hmdb_df = hmdb_df.dropna(subset=[\"HMDBSMILES\"])\n",
    "\n",
    "                            # 1 SGHM\n",
    "                            if (\n",
    "                                len(sirius_df) > 0\n",
    "                                and len(gnps_df) > 0\n",
    "                                and len(mbank_df) > 0\n",
    "                                and len(hmdb_df) > 0\n",
    "                            ):\n",
    "                                mbank_df[\"rank_ids\"] = [\n",
    "                                    \"M_\" + str(s + 1) for s in range(len(mbank_df))\n",
    "                                ]\n",
    "\n",
    "                                gnps_df[\"rank_ids\"] = [\n",
    "                                    \"G_\" + str(s + 1) for s in range(len(gnps_df))\n",
    "                                ]\n",
    "\n",
    "                                hmdb_df[\"rank_ids\"] = [\n",
    "                                    \"H_\" + str(s + 1) for s in range(len(hmdb_df))\n",
    "                                ]\n",
    "\n",
    "                                sirius_df[\"rank_ids\"] = [\n",
    "                                    \"S_\" + str(s) for s in sirius_df[\"rank\"]\n",
    "                                ]\n",
    "                                sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "                                source_l1 = [\n",
    "                                    *(list(sirius_df[\"Source\"])),\n",
    "                                    *(list(gnps_df[\"Source\"])),\n",
    "                                    *(list(mbank_df[\"Source\"])),\n",
    "                                    *(list(hmdb_df[\"Source\"])),\n",
    "                                ]\n",
    "\n",
    "                                rank_l2 = [\n",
    "                                    *(list(sirius_df[\"rank_ids\"])),\n",
    "                                    *(list(gnps_df[\"rank_ids\"])),\n",
    "                                    *(list(mbank_df[\"rank_ids\"])),\n",
    "                                    *(list(hmdb_df[\"rank_ids\"])),\n",
    "                                ]\n",
    "\n",
    "                                smiles_l3 = [\n",
    "                                    *(list(sirius_df[\"smiles\"])),\n",
    "                                    *(list(gnps_df[\"GNPSSMILES\"])),\n",
    "                                    *(list(mbank_df[\"MBSMILES\"])),\n",
    "                                    *(list(hmdb_df[\"HMDBSMILES\"])),\n",
    "                                ]\n",
    "\n",
    "                                sm = pd.DataFrame(\n",
    "                                    list(zip(source_l1, rank_l2, smiles_l3)),\n",
    "                                    columns=[\"Source\", \"ranks\", \"SMILES\"],\n",
    "                                )\n",
    "\n",
    "                                df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                                df_edge.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_ChemMNedges.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "                                one_candidate = one_candidate_selection(\n",
    "                                    sm,\n",
    "                                    sirius_df=sirius_df,\n",
    "                                    mbank_df=mbank_df,\n",
    "                                    gnps_df=gnps_df,\n",
    "                                    hmdb_df=hmdb_df,\n",
    "                                    Source=\"SGHM\",\n",
    "                                )\n",
    "                                one_candidate.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_one_candidate_list.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "                                candidates_with_counts = add_count_column(one_candidate)\n",
    "\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 4:\n",
    "                                    sources_4(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 3:\n",
    "                                    sources_3(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 2:\n",
    "                                    sources_2(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 1:\n",
    "                                    sources_1(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "\n",
    "                            # 2 SGM\n",
    "                            elif (\n",
    "                                len(sirius_df) > 0\n",
    "                                and len(gnps_df) > 0\n",
    "                                and len(mbank_df) > 0\n",
    "                                and len(hmdb_df) == 0\n",
    "                            ):\n",
    "\n",
    "                                mbank_df[\"rank_ids\"] = [\n",
    "                                    \"M_\" + str(s + 1) for s in range(len(mbank_df))\n",
    "                                ]\n",
    "\n",
    "                                gnps_df[\"rank_ids\"] = [\n",
    "                                    \"G_\" + str(s + 1) for s in range(len(gnps_df))\n",
    "                                ]\n",
    "\n",
    "                                sirius_df[\"rank_ids\"] = [\n",
    "                                    \"S_\" + str(s) for s in sirius_df[\"rank\"]\n",
    "                                ]\n",
    "                                sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "                                source_l1 = [\n",
    "                                    *(list(sirius_df[\"Source\"])),\n",
    "                                    *(list(gnps_df[\"Source\"])),\n",
    "                                    *(list(mbank_df[\"Source\"])),\n",
    "                                ]\n",
    "\n",
    "                                rank_l2 = [\n",
    "                                    *(list(sirius_df[\"rank_ids\"])),\n",
    "                                    *(list(gnps_df[\"rank_ids\"])),\n",
    "                                    *(list(mbank_df[\"rank_ids\"])),\n",
    "                                ]\n",
    "\n",
    "                                smiles_l3 = [\n",
    "                                    *(list(sirius_df[\"smiles\"])),\n",
    "                                    *(list(gnps_df[\"GNPSSMILES\"])),\n",
    "                                    *(list(mbank_df[\"MBSMILES\"])),\n",
    "                                ]\n",
    "\n",
    "                                sm = pd.DataFrame(\n",
    "                                    list(zip(source_l1, rank_l2, smiles_l3)),\n",
    "                                    columns=[\"Source\", \"ranks\", \"SMILES\"],\n",
    "                                )\n",
    "\n",
    "                                df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                                df_edge.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_ChemMNedges.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "                                one_candidate = one_candidate_selection(\n",
    "                                    sm,\n",
    "                                    sirius_df=sirius_df,\n",
    "                                    mbank_df=mbank_df,\n",
    "                                    gnps_df=gnps_df,\n",
    "                                    # hmdb_df = hmdb_df,\n",
    "                                    Source=\"SGM\",\n",
    "                                )\n",
    "                                one_candidate.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_one_candidate_list.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "                                candidates_with_counts = add_count_column(one_candidate)\n",
    "\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 3:\n",
    "                                    sources_3(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 2:\n",
    "                                    sources_2(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 1:\n",
    "                                    sources_1(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "\n",
    "                            # 3 SHM\n",
    "                            elif (\n",
    "                                len(sirius_df) > 0\n",
    "                                and len(gnps_df) == 0\n",
    "                                and len(mbank_df) > 0\n",
    "                                and len(hmdb_df) > 0\n",
    "                            ):\n",
    "\n",
    "                                mbank_df[\"rank_ids\"] = [\n",
    "                                    \"M_\" + str(s + 1) for s in range(len(mbank_df))\n",
    "                                ]\n",
    "\n",
    "                                # gnps_df[\"rank_ids\"] = [\"G_\" + str(s+1) for s in range(len(gnps_df))]\n",
    "\n",
    "                                hmdb_df[\"rank_ids\"] = [\n",
    "                                    \"H_\" + str(s + 1) for s in range(len(hmdb_df))\n",
    "                                ]\n",
    "\n",
    "                                sirius_df[\"rank_ids\"] = [\n",
    "                                    \"S_\" + str(s) for s in sirius_df[\"rank\"]\n",
    "                                ]\n",
    "                                sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "                                source_l1 = [\n",
    "                                    *(list(sirius_df[\"Source\"])),\n",
    "                                    *(list(mbank_df[\"Source\"])),\n",
    "                                    *(list(hmdb_df[\"Source\"])),\n",
    "                                ]\n",
    "\n",
    "                                rank_l2 = [\n",
    "                                    *(list(sirius_df[\"rank_ids\"])),\n",
    "                                    *(list(mbank_df[\"rank_ids\"])),\n",
    "                                    *(list(hmdb_df[\"rank_ids\"])),\n",
    "                                ]\n",
    "\n",
    "                                smiles_l3 = [\n",
    "                                    *(list(sirius_df[\"smiles\"])),\n",
    "                                    *(list(mbank_df[\"MBSMILES\"])),\n",
    "                                    *(list(hmdb_df[\"HMDBSMILES\"])),\n",
    "                                ]\n",
    "\n",
    "                                sm = pd.DataFrame(\n",
    "                                    list(zip(source_l1, rank_l2, smiles_l3)),\n",
    "                                    columns=[\"Source\", \"ranks\", \"SMILES\"],\n",
    "                                )\n",
    "\n",
    "                                df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                                df_edge.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_ChemMNedges.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "                                one_candidate = one_candidate_selection(\n",
    "                                    sm,\n",
    "                                    sirius_df=sirius_df,\n",
    "                                    mbank_df=mbank_df,\n",
    "                                    # gnps_df = gnps_df ,\n",
    "                                    hmdb_df=hmdb_df,\n",
    "                                    Source=\"SHM\",\n",
    "                                )\n",
    "                                one_candidate.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_one_candidate_list.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "                                candidates_with_counts = add_count_column(one_candidate)\n",
    "\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 3:\n",
    "                                    sources_3(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 2:\n",
    "                                    sources_2(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 1:\n",
    "                                    sources_1(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "\n",
    "                            # 4 SGH\n",
    "                            elif (\n",
    "                                len(sirius_df) > 0\n",
    "                                and len(gnps_df) > 0\n",
    "                                and len(mbank_df) == 0\n",
    "                                and len(hmdb_df) > 0\n",
    "                            ):\n",
    "                                # mbank_df[\"rank_ids\"] = [\"M_\" + str(s+1) for s in range(len(mbank_df))]\n",
    "\n",
    "                                gnps_df[\"rank_ids\"] = [\n",
    "                                    \"G_\" + str(s + 1) for s in range(len(gnps_df))\n",
    "                                ]\n",
    "\n",
    "                                hmdb_df[\"rank_ids\"] = [\n",
    "                                    \"H_\" + str(s + 1) for s in range(len(hmdb_df))\n",
    "                                ]\n",
    "\n",
    "                                sirius_df[\"rank_ids\"] = [\n",
    "                                    \"S_\" + str(s) for s in sirius_df[\"rank\"]\n",
    "                                ]\n",
    "                                sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "                                source_l1 = [\n",
    "                                    *(list(sirius_df[\"Source\"])),\n",
    "                                    *(list(gnps_df[\"Source\"])),\n",
    "                                    *(list(hmdb_df[\"Source\"])),\n",
    "                                ]\n",
    "\n",
    "                                rank_l2 = [\n",
    "                                    *(list(sirius_df[\"rank_ids\"])),\n",
    "                                    *(list(gnps_df[\"rank_ids\"])),\n",
    "                                    *(list(hmdb_df[\"rank_ids\"])),\n",
    "                                ]\n",
    "\n",
    "                                smiles_l3 = [\n",
    "                                    *(list(sirius_df[\"smiles\"])),\n",
    "                                    *(list(gnps_df[\"GNPSSMILES\"])),\n",
    "                                    *(list(hmdb_df[\"HMDBSMILES\"])),\n",
    "                                ]\n",
    "\n",
    "                                sm = pd.DataFrame(\n",
    "                                    list(zip(source_l1, rank_l2, smiles_l3)),\n",
    "                                    columns=[\"Source\", \"ranks\", \"SMILES\"],\n",
    "                                )\n",
    "\n",
    "                                df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                                df_edge.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_ChemMNedges.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "                                one_candidate = one_candidate_selection(\n",
    "                                    sm,\n",
    "                                    sirius_df=sirius_df,\n",
    "                                    # mbank_df = mbank_df,\n",
    "                                    gnps_df=gnps_df,\n",
    "                                    hmdb_df=hmdb_df,\n",
    "                                    Source=\"SGH\",\n",
    "                                )\n",
    "                                one_candidate.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_one_candidate_list.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "                                candidates_with_counts = add_count_column(one_candidate)\n",
    "\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 3:\n",
    "                                    sources_3(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 2:\n",
    "                                    sources_2(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 1:\n",
    "                                    sources_1(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "\n",
    "\n",
    "                            # 5 GHM\n",
    "                            elif (\n",
    "                                len(sirius_df) == 0\n",
    "                                and len(gnps_df) > 0\n",
    "                                and len(mbank_df) > 0\n",
    "                                and len(hmdb_df) > 0\n",
    "                            ):\n",
    "                                mbank_df[\"rank_ids\"] = [\n",
    "                                    \"M_\" + str(s + 1) for s in range(len(mbank_df))\n",
    "                                ]\n",
    "\n",
    "                                gnps_df[\"rank_ids\"] = [\n",
    "                                    \"G_\" + str(s + 1) for s in range(len(gnps_df))\n",
    "                                ]\n",
    "\n",
    "                                hmdb_df[\"rank_ids\"] = [\n",
    "                                    \"H_\" + str(s + 1) for s in range(len(hmdb_df))\n",
    "                                ]\n",
    "\n",
    "                                # sirius_df[\"rank_ids\"] = [\"S_\" + str(s) for s in sirius_df[\"rank\"]]\n",
    "                                # sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "                                source_l1 = [\n",
    "                                    *(list(gnps_df[\"Source\"])),\n",
    "                                    *(list(mbank_df[\"Source\"])),\n",
    "                                    *(list(hmdb_df[\"Source\"])),\n",
    "                                ]\n",
    "\n",
    "                                rank_l2 = [\n",
    "                                    *(list(gnps_df[\"rank_ids\"])),\n",
    "                                    *(list(mbank_df[\"rank_ids\"])),\n",
    "                                    *(list(hmdb_df[\"rank_ids\"])),\n",
    "                                ]\n",
    "\n",
    "                                smiles_l3 = [\n",
    "                                    *(list(gnps_df[\"GNPSSMILES\"])),\n",
    "                                    *(list(mbank_df[\"MBSMILES\"])),\n",
    "                                    *(list(hmdb_df[\"HMDBSMILES\"])),\n",
    "                                ]\n",
    "\n",
    "                                sm = pd.DataFrame(\n",
    "                                    list(zip(source_l1, rank_l2, smiles_l3)),\n",
    "                                    columns=[\"Source\", \"ranks\", \"SMILES\"],\n",
    "                                )\n",
    "\n",
    "                                df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                                df_edge.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_ChemMNedges.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "                                one_candidate = one_candidate_selection(\n",
    "                                    sm,\n",
    "                                    # sirius_df = sirius_df,\n",
    "                                    mbank_df=mbank_df,\n",
    "                                    gnps_df=gnps_df,\n",
    "                                    hmdb_df=hmdb_df,\n",
    "                                    Source=\"GHM\",\n",
    "                                )\n",
    "                                one_candidate.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_one_candidate_list.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "\n",
    "                                candidates_with_counts = add_count_column(one_candidate)\n",
    "\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 3:\n",
    "                                    sources_3(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 2:\n",
    "                                    sources_2(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 1:\n",
    "                                    sources_1(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "\n",
    "                            # 6 SG\n",
    "                            elif (\n",
    "                                len(sirius_df) > 0\n",
    "                                and len(gnps_df) > 0\n",
    "                                and len(mbank_df) == 0\n",
    "                                and len(hmdb_df) == 0\n",
    "                            ):\n",
    "                                # mbank_df[\"rank_ids\"] = [\"M_\" + str(s+1) for s in range(len(mbank_df))]\n",
    "\n",
    "                                gnps_df[\"rank_ids\"] = [\n",
    "                                    \"G_\" + str(s + 1) for s in range(len(gnps_df))\n",
    "                                ]\n",
    "\n",
    "                                sirius_df[\"rank_ids\"] = [\n",
    "                                    \"S_\" + str(s) for s in sirius_df[\"rank\"]\n",
    "                                ]\n",
    "                                sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "                                source_l1 = [\n",
    "                                    *(list(sirius_df[\"Source\"])),\n",
    "                                    *(list(gnps_df[\"Source\"])),\n",
    "                                ]\n",
    "                                # ,*(list(mbank_df[\"Source\"]))]\n",
    "\n",
    "                                rank_l2 = [\n",
    "                                    *(list(sirius_df[\"rank_ids\"])),\n",
    "                                    *(list(gnps_df[\"rank_ids\"])),\n",
    "                                ]\n",
    "                                # ,*(list(mbank_df[\"rank_ids\"]))]\n",
    "\n",
    "                                smiles_l3 = [\n",
    "                                    *(list(sirius_df[\"smiles\"])),\n",
    "                                    *(list(gnps_df[\"GNPSSMILES\"])),\n",
    "                                ]\n",
    "                                # ,*(list(mbank_df[\"MBSMILES\"]))]\n",
    "\n",
    "                                sm = pd.DataFrame(\n",
    "                                    list(zip(source_l1, rank_l2, smiles_l3)),\n",
    "                                    columns=[\"Source\", \"ranks\", \"SMILES\"],\n",
    "                                )\n",
    "\n",
    "                                df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                                df_edge.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_ChemMNedges.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "\n",
    "                                one_candidate = one_candidate_selection(\n",
    "                                    sm,\n",
    "                                    sirius_df=sirius_df,\n",
    "                                    # mbank_df = mbank_df,\n",
    "                                    gnps_df=gnps_df,\n",
    "                                    # hmdb_df = hmdb_df,\n",
    "                                    Source=\"SG\",\n",
    "                                )\n",
    "                                one_candidate.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_one_candidate_list.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "                                candidates_with_counts = add_count_column(one_candidate)\n",
    "\n",
    "\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 2:\n",
    "                                    sources_2(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 1:\n",
    "                                    sources_1(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "\n",
    "                            # 7 SH\n",
    "                            elif (\n",
    "                                len(sirius_df) > 0\n",
    "                                and len(gnps_df) == 0\n",
    "                                and len(mbank_df) == 0\n",
    "                                and len(hmdb_df) > 0\n",
    "                            ):\n",
    "                                # mbank_df[\"rank_ids\"] = [\"M_\" + str(s+1) for s in range(len(mbank_df))]\n",
    "\n",
    "                                # gnps_df[\"rank_ids\"] = [\"G_\" + str(s+1) for s in range(len(gnps_df))]\n",
    "\n",
    "                                hmdb_df[\"rank_ids\"] = [\n",
    "                                    \"H_\" + str(s + 1) for s in range(len(hmdb_df))\n",
    "                                ]\n",
    "\n",
    "                                sirius_df[\"rank_ids\"] = [\n",
    "                                    \"S_\" + str(s) for s in sirius_df[\"rank\"]\n",
    "                                ]\n",
    "                                sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "                                source_l1 = [\n",
    "                                    *(list(sirius_df[\"Source\"])),\n",
    "                                    *(list(hmdb_df[\"Source\"])),\n",
    "                                ]\n",
    "\n",
    "                                rank_l2 = [\n",
    "                                    *(list(sirius_df[\"rank_ids\"])),\n",
    "                                    *(list(hmdb_df[\"rank_ids\"])),\n",
    "                                ]\n",
    "\n",
    "                                smiles_l3 = [\n",
    "                                    *(list(sirius_df[\"smiles\"])),\n",
    "                                    *(list(hmdb_df[\"HMDBSMILES\"])),\n",
    "                                ]\n",
    "\n",
    "                                sm = pd.DataFrame(\n",
    "                                    list(zip(source_l1, rank_l2, smiles_l3)),\n",
    "                                    columns=[\"Source\", \"ranks\", \"SMILES\"],\n",
    "                                )\n",
    "                                df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                                df_edge.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_ChemMNedges.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "\n",
    "                                one_candidate = one_candidate_selection(\n",
    "                                    sm,\n",
    "                                    sirius_df=sirius_df,\n",
    "                                    # mbank_df = mbank_df,\n",
    "                                    # gnps_df = gnps_df ,\n",
    "                                    hmdb_df=hmdb_df,\n",
    "                                    Source=\"SH\",\n",
    "                                )\n",
    "                                one_candidate.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_one_candidate_list.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "                                candidates_with_counts = add_count_column(one_candidate)\n",
    "\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 2:\n",
    "                                    sources_2(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 1:\n",
    "                                    sources_1(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "\n",
    "                            # 8 SM\n",
    "                            elif (\n",
    "                                len(sirius_df) > 0\n",
    "                                and len(gnps_df) == 0\n",
    "                                and len(mbank_df) > 0\n",
    "                                and len(hmdb_df) == 0\n",
    "                            ):\n",
    "                                mbank_df[\"rank_ids\"] = [\n",
    "                                    \"M_\" + str(s + 1) for s in range(len(mbank_df))\n",
    "                                ]\n",
    "\n",
    "                                # gnps_df[\"rank_ids\"] = [\"G_\" + str(s+1) for s in range(len(gnps_df))]\n",
    "\n",
    "                                # hmdb_df[\"rank_ids\"] = [\"H_\" + str(s+1) for s in range(len(hmdb_df))]\n",
    "\n",
    "                                sirius_df[\"rank_ids\"] = [\n",
    "                                    \"S_\" + str(s) for s in sirius_df[\"rank\"]\n",
    "                                ]\n",
    "                                sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "                                source_l1 = [\n",
    "                                    *(list(sirius_df[\"Source\"])),\n",
    "                                    *(list(mbank_df[\"Source\"])),\n",
    "                                ]\n",
    "\n",
    "                                rank_l2 = [\n",
    "                                    *(list(sirius_df[\"rank_ids\"])),\n",
    "                                    *(list(mbank_df[\"rank_ids\"])),\n",
    "                                ]\n",
    "\n",
    "                                smiles_l3 = [\n",
    "                                    *(list(sirius_df[\"smiles\"])),\n",
    "                                    *(list(mbank_df[\"MBSMILES\"])),\n",
    "                                ]\n",
    "\n",
    "                                sm = pd.DataFrame(\n",
    "                                    list(zip(source_l1, rank_l2, smiles_l3)),\n",
    "                                    columns=[\"Source\", \"ranks\", \"SMILES\"],\n",
    "                                )\n",
    "\n",
    "                                df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                                df_edge.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_ChemMNedges.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "\n",
    "                                one_candidate = one_candidate_selection(\n",
    "                                    sm,\n",
    "                                    sirius_df=sirius_df,\n",
    "                                    mbank_df=mbank_df,\n",
    "                                    # gnps_df = gnps_df ,\n",
    "                                    # hmdb_df = hmdb_df,\n",
    "                                    Source=\"SM\",\n",
    "                                )\n",
    "                                one_candidate.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_one_candidate_list.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "                                candidates_with_counts = add_count_column(one_candidate)\n",
    "\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 2:\n",
    "                                    sources_2(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 1:\n",
    "                                    sources_1(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "\n",
    "                            # 9 GM\n",
    "                            elif (\n",
    "                                len(sirius_df) == 0\n",
    "                                and len(gnps_df) > 0\n",
    "                                and len(mbank_df) > 0\n",
    "                                and len(hmdb_df) == 0\n",
    "                            ):\n",
    "                                mbank_df[\"rank_ids\"] = [\n",
    "                                    \"M_\" + str(s + 1) for s in range(len(mbank_df))\n",
    "                                ]\n",
    "\n",
    "                                gnps_df[\"rank_ids\"] = [\n",
    "                                    \"G_\" + str(s + 1) for s in range(len(gnps_df))\n",
    "                                ]\n",
    "\n",
    "                                # hmdb_df[\"rank_ids\"] = [\"H_\" + str(s+1) for s in range(len(hmdb_df))]\n",
    "\n",
    "                                # sirius_df[\"rank_ids\"] = [\"S_\" + str(s) for s in sirius_df[\"rank\"]]\n",
    "                                # sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "                                source_l1 = [\n",
    "                                    *(list(mbank_df[\"Source\"])),\n",
    "                                    *(list(gnps_df[\"Source\"])),\n",
    "                                ]\n",
    "\n",
    "                                rank_l2 = [\n",
    "                                    *(list(mbank_df[\"rank_ids\"])),\n",
    "                                    *(list(gnps_df[\"rank_ids\"])),\n",
    "                                ]\n",
    "\n",
    "                                smiles_l3 = [\n",
    "                                    *(list(mbank_df[\"MBSMILES\"])),\n",
    "                                    *(list(gnps_df[\"GNPSSMILES\"])),\n",
    "                                ]\n",
    "\n",
    "                                sm = pd.DataFrame(\n",
    "                                    list(zip(source_l1, rank_l2, smiles_l3)),\n",
    "                                    columns=[\"Source\", \"ranks\", \"SMILES\"],\n",
    "                                )\n",
    "\n",
    "                                df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                                df_edge.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_ChemMNedges.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "\n",
    "                                one_candidate = one_candidate_selection(\n",
    "                                    sm,\n",
    "                                    # sirius_df = sirius_df,\n",
    "                                    mbank_df=mbank_df,\n",
    "                                    gnps_df=gnps_df,\n",
    "                                    # hmdb_df = hmdb_df,\n",
    "                                    Source=\"GM\",\n",
    "                                )\n",
    "                                one_candidate.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_one_candidate_list.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "                                candidates_with_counts = add_count_column(one_candidate)\n",
    "\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 2:\n",
    "                                    sources_2(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 1:\n",
    "                                    sources_1(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "\n",
    "                            # 10 GH\n",
    "                            elif (\n",
    "                                len(sirius_df) == 0\n",
    "                                and len(gnps_df) > 0\n",
    "                                and len(mbank_df) == 0\n",
    "                                and len(hmdb_df) > 0\n",
    "                            ):\n",
    "                                # mbank_df[\"rank_ids\"] = [\"M_\" + str(s+1) for s in range(len(mbank_df))]\n",
    "\n",
    "                                gnps_df[\"rank_ids\"] = [\n",
    "                                    \"G_\" + str(s + 1) for s in range(len(gnps_df))\n",
    "                                ]\n",
    "\n",
    "                                hmdb_df[\"rank_ids\"] = [\n",
    "                                    \"H_\" + str(s + 1) for s in range(len(hmdb_df))\n",
    "                                ]\n",
    "\n",
    "                                # sirius_df[\"rank_ids\"] = [\"S_\" + str(s) for s in sirius_df[\"rank\"]]\n",
    "                                # sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "                                source_l1 = [\n",
    "                                    *(list(gnps_df[\"Source\"])),\n",
    "                                    *(list(hmdb_df[\"Source\"])),\n",
    "                                ]\n",
    "\n",
    "                                rank_l2 = [\n",
    "                                    *(list(gnps_df[\"rank_ids\"])),\n",
    "                                    *(list(hmdb_df[\"rank_ids\"])),\n",
    "                                ]\n",
    "\n",
    "                                smiles_l3 = [\n",
    "                                    *(list(gnps_df[\"GNPSSMILES\"])),\n",
    "                                    *(list(hmdb_df[\"HMDBSMILES\"])),\n",
    "                                ]\n",
    "\n",
    "                                sm = pd.DataFrame(\n",
    "                                    list(zip(source_l1, rank_l2, smiles_l3)),\n",
    "                                    columns=[\"Source\", \"ranks\", \"SMILES\"],\n",
    "                                )\n",
    "                                df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                                df_edge.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_ChemMNedges.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "                                one_candidate = one_candidate_selection(\n",
    "                                    sm,\n",
    "                                    # sirius_df = sirius_df,\n",
    "                                    # mbank_df = mbank_df,\n",
    "                                    gnps_df=gnps_df,\n",
    "                                    hmdb_df=hmdb_df,\n",
    "                                    Source=\"GH\",\n",
    "                                )\n",
    "                                one_candidate.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_one_candidate_list.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "                                candidates_with_counts = add_count_column(one_candidate)\n",
    "\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 2:\n",
    "                                    sources_2(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 1:\n",
    "                                    sources_1(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "                            # 11 HM\n",
    "                            elif (\n",
    "                                len(sirius_df) == 0\n",
    "                                and len(gnps_df) == 0\n",
    "                                and len(mbank_df) > 0\n",
    "                                and len(hmdb_df) > 0\n",
    "                            ):\n",
    "                                mbank_df[\"rank_ids\"] = [\n",
    "                                    \"M_\" + str(s + 1) for s in range(len(mbank_df))\n",
    "                                ]\n",
    "\n",
    "                                # gnps_df[\"rank_ids\"] = [\"G_\" + str(s+1) for s in range(len(gnps_df))]\n",
    "\n",
    "                                hmdb_df[\"rank_ids\"] = [\n",
    "                                    \"H_\" + str(s + 1) for s in range(len(hmdb_df))\n",
    "                                ]\n",
    "\n",
    "                                # sirius_df[\"rank_ids\"] = [\"S_\" + str(s) for s in sirius_df[\"rank\"]]\n",
    "                                # sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "                                source_l1 = [\n",
    "                                    *(list(mbank_df[\"Source\"])),\n",
    "                                    *(list(hmdb_df[\"Source\"])),\n",
    "                                ]\n",
    "\n",
    "                                rank_l2 = [\n",
    "                                    *(list(mbank_df[\"rank_ids\"])),\n",
    "                                    *(list(hmdb_df[\"rank_ids\"])),\n",
    "                                ]\n",
    "\n",
    "                                smiles_l3 = [\n",
    "                                    *(list(mbank_df[\"MBSMILES\"])),\n",
    "                                    *(list(hmdb_df[\"HMDBSMILES\"])),\n",
    "                                ]\n",
    "\n",
    "                                sm = pd.DataFrame(\n",
    "                                    list(zip(source_l1, rank_l2, smiles_l3)),\n",
    "                                    columns=[\"Source\", \"ranks\", \"SMILES\"],\n",
    "                                )\n",
    "\n",
    "                                df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                                df_edge.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_ChemMNedges.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "                                one_candidate = one_candidate_selection(\n",
    "                                    sm,\n",
    "                                    # sirius_df = sirius_df,\n",
    "                                    mbank_df=mbank_df,\n",
    "                                    # gnps_df = gnps_df ,\n",
    "                                    hmdb_df=hmdb_df,\n",
    "                                    Source=\"HM\",\n",
    "                                )\n",
    "                                one_candidate.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_one_candidate_list.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "                                candidates_with_counts = add_count_column(one_candidate)\n",
    "\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 2:\n",
    "                                    sources_2(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 1:\n",
    "                                    sources_1(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "\n",
    "                            # S\n",
    "                            elif (\n",
    "                                len(sirius_df) > 0\n",
    "                                and len(gnps_df) == 0\n",
    "                                and len(mbank_df) == 0\n",
    "                                and len(hmdb_df) == 0\n",
    "                            ):\n",
    "\n",
    "                                sirius_df[\"rank_ids\"] = [\n",
    "                                    \"S_\" + str(s) for s in sirius_df[\"rank\"]\n",
    "                                ]\n",
    "                                sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "                                source_l1 = [*(list(sirius_df[\"Source\"]))]\n",
    "\n",
    "                                rank_l2 = [*(list(sirius_df[\"rank_ids\"]))]\n",
    "\n",
    "                                smiles_l3 = [*(list(sirius_df[\"smiles\"]))]\n",
    "\n",
    "                                sm = pd.DataFrame(\n",
    "                                    list(zip(source_l1, rank_l2, smiles_l3)),\n",
    "                                    columns=[\"Source\", \"ranks\", \"SMILES\"],\n",
    "                                )\n",
    "\n",
    "                                df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                                df_edge.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_ChemMNedges.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "\n",
    "                                one_candidate = one_candidate_selection(\n",
    "                                    sm,\n",
    "                                    sirius_df=sirius_df,\n",
    "                                    # mbank_df = mbank_df,\n",
    "                                    # gnps_df = gnps_df ,\n",
    "                                    # hmdb_df = hmdb_df,\n",
    "                                    Source=\"S\",\n",
    "                                )\n",
    "                                one_candidate.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_one_candidate_list.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "                                candidates_with_counts = add_count_column(one_candidate)\n",
    "\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 1:\n",
    "                                    sources_1(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "                            # G\n",
    "                            elif (\n",
    "                                len(sirius_df) == 0\n",
    "                                and len(gnps_df) > 0\n",
    "                                and len(mbank_df) == 0\n",
    "                                and len(hmdb_df) == 0\n",
    "                            ):\n",
    "                                gnps_df[\"rank_ids\"] = [\n",
    "                                    \"G_\" + str(s + 1) for s in range(len(gnps_df))\n",
    "                                ]\n",
    "\n",
    "                                source_l1 = [*(list(gnps_df[\"Source\"]))]\n",
    "\n",
    "                                rank_l2 = [*(list(gnps_df[\"rank_ids\"]))]\n",
    "\n",
    "                                smiles_l3 = [*(list(gnps_df[\"GNPSSMILES\"]))]\n",
    "\n",
    "                                sm = pd.DataFrame(\n",
    "                                    list(zip(source_l1, rank_l2, smiles_l3)),\n",
    "                                    columns=[\"Source\", \"ranks\", \"SMILES\"],\n",
    "                                )\n",
    "\n",
    "                                df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                                df_edge.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_ChemMNedges.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "\n",
    "                                one_candidate = one_candidate_selection(\n",
    "                                    sm,\n",
    "                                    # sirius_df = sirius_df,\n",
    "                                    # mbank_df = mbank_df,\n",
    "                                    gnps_df=gnps_df,\n",
    "                                    # hmdb_df = hmdb_df,\n",
    "                                    Source=\"G\",\n",
    "                                )\n",
    "                                one_candidate.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_one_candidate_list.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "                                candidates_with_counts = add_count_column(one_candidate)\n",
    "\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 1:\n",
    "                                    sources_1(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "                            # M\n",
    "                            elif (\n",
    "                                len(sirius_df) == 0\n",
    "                                and len(gnps_df) == 0\n",
    "                                and len(mbank_df) > 0\n",
    "                                and len(hmdb_df) == 0\n",
    "                            ):\n",
    "                                mbank_df[\"rank_ids\"] = [\n",
    "                                    \"M_\" + str(s + 1) for s in range(len(mbank_df))\n",
    "                                ]\n",
    "\n",
    "                                source_l1 = [*(list(mbank_df[\"Source\"]))]\n",
    "\n",
    "                                rank_l2 = [*(list(mbank_df[\"rank_ids\"]))]\n",
    "\n",
    "                                smiles_l3 = [*(list(mbank_df[\"smiles\"]))]\n",
    "\n",
    "                                sm = pd.DataFrame(\n",
    "                                    list(zip(source_l1, rank_l2, smiles_l3)),\n",
    "                                    columns=[\"Source\", \"ranks\", \"SMILES\"],\n",
    "                                )\n",
    "\n",
    "                                df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                                df_edge.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_ChemMNedges.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "\n",
    "                                one_candidate = one_candidate_selection(\n",
    "                                    sm,\n",
    "                                    # sirius_df = sirius_df,\n",
    "                                    mbank_df=mbank_df,\n",
    "                                    # gnps_df = gnps_df ,\n",
    "                                    # hmdb_df = hmdb_df,\n",
    "                                    Source=\"M\",\n",
    "                                )\n",
    "                                one_candidate.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_one_candidate_list.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "                                candidates_with_counts = add_count_column(one_candidate)\n",
    "\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 1:\n",
    "                                    sources_1(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "                            # H\n",
    "                            elif (\n",
    "                                len(sirius_df) == 0\n",
    "                                and len(gnps_df) == 0\n",
    "                                and len(mbank_df) == 0\n",
    "                                and len(hmdb_df) > 0\n",
    "                            ):\n",
    "                                hmdb_df[\"rank_ids\"] = [\n",
    "                                    \"H_\" + str(s + 1) for s in range(len(hmdb_df))\n",
    "                                ]\n",
    "\n",
    "                                source_l1 = [*(list(hmdb_df[\"Source\"]))]\n",
    "\n",
    "                                rank_l2 = [*(list(hmdb_df[\"rank_ids\"]))]\n",
    "\n",
    "                                smiles_l3 = [*(list(hmdb_df[\"HMDBSMILES\"]))]\n",
    "\n",
    "                                sm = pd.DataFrame(\n",
    "                                    list(zip(source_l1, rank_l2, smiles_l3)),\n",
    "                                    columns=[\"Source\", \"ranks\", \"SMILES\"],\n",
    "                                )\n",
    "\n",
    "                                df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                                df_edge.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_ChemMNedges.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "\n",
    "                                one_candidate = one_candidate_selection(\n",
    "                                    sm,\n",
    "                                    #                                                                         sirius_df = sirius_df,\n",
    "                                    #                                                                         mbank_df = mbank_df,\n",
    "                                    #                                                                         gnps_df = gnps_df ,\n",
    "                                    hmdb_df=hmdb_df,\n",
    "                                    Source=\"H\",\n",
    "                                )\n",
    "                                one_candidate.to_csv(\n",
    "                                    input_dir\n",
    "                                    + \"/\"\n",
    "                                    + entry\n",
    "                                    + \"/\"\n",
    "                                    + str(merged_df[\"premz\"][mer])\n",
    "                                    + \"_one_candidate_list.tsv\",\n",
    "                                    sep=\"\\t\",\n",
    "                                )\n",
    "                                candidates_with_counts = add_count_column(one_candidate)\n",
    "\n",
    "                                if max(candidates_with_counts[\"Count\"]) == 1:\n",
    "                                    sources_1(candidates_with_counts, merged_df, mer, sirius_df)\n",
    "                        \n",
    "                        if standards:\n",
    "                            if not isNaN(merged_df[\"SMILES\"][mer]):\n",
    "                        \n",
    "                                merged_df.loc[mer, \"MSILevel\"] = 1\n",
    "                            \n",
    "                    merged_df.to_csv(\n",
    "                        input_dir\n",
    "                        + \"/\"\n",
    "                        + entry\n",
    "                        + \"/\"\n",
    "                        + \"mergedResults-with-one-Candidates.csv\"\n",
    "                    )\n",
    "                    #print(merged_df)\n",
    "                    merged_df = checkSMILES_validity(input_dir, resultcsv = input_dir\n",
    "                        + \"/\"\n",
    "                        + entry\n",
    "                        + \"/\"\n",
    "                        + \"mergedResults-with-one-Candidates.csv\")\n",
    "                    merged_df.to_csv(\n",
    "                        input_dir\n",
    "                        + \"/\"\n",
    "                        + entry\n",
    "                        + \"/\"\n",
    "                        + \"mergedResults-with-one-Candidates.csv\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9198dbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_results(input_dir):\n",
    "    names = []\n",
    "    # entry is all files and folders in input_dir\n",
    "    for entry in os.listdir(input_dir):\n",
    "        # if the entry is also a directory\n",
    "        if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "\n",
    "            # reach spectra_dereplication folder\n",
    "            merged_file_res = input_dir+ \"/\"+ entry+ \"/mergedResults-with-one-Candidates.csv\"\n",
    "\n",
    "            if os.path.exists(merged_file_res):\n",
    "                merged_csv = pd.read_csv(merged_file_res)\n",
    "                names.append(merged_csv)\n",
    "    merged = (pd.concat(names, ignore_index=True))\n",
    "    # Select the ones you want\n",
    "    merged = merged[['id_X_x', 'premz', 'rtmed', 'rtmean',\n",
    "           'int', 'col_eng', 'pol', 'rtmin', 'rtmax', 'source_file', 'Formula',\n",
    "                    'SMILES', 'IUPAC', 'synonyms', 'AnnotationSources', 'AnnotationCount',\n",
    "           'MSILevel', 'MCSS', 'subclass', 'class', 'superclass', 'ClassificationSource']]\n",
    "    merged.to_csv(input_dir + \"/final_candidates.csv\")\n",
    "    return(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "961a3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison with a list of SMILES from any Source\n",
    "def SMILESscreening(input_dir, resultcsv, complist, listname):\n",
    "   \n",
    "    \"\"\"SMILESscreening takes a list of SMILES\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML\n",
    "    files and their respective result directories are stored.\n",
    "\n",
    "    resultcsv: df from combine_CuratedR or checkSMILES_validity or classification\n",
    "    complist: list of /n separated txt file conyaining smiles on each line\n",
    "    listname: name of the list of compounds\n",
    "\n",
    "    Returns:\n",
    "    dataframe: comparison with another list of compounds\n",
    "    csv: \"MetabolomicsResults/final_curation_with_validSMILES.csv\"\n",
    "\n",
    "    Usage:\n",
    "    checkSMILES_validity(input_dir = \"usr/project/\", results)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    results = pd.read_csv(resultcsv)\n",
    "    with open(complist, \"r\") as text_file:\n",
    "        cd = text_file.read().split(\"\\n\")\n",
    "\n",
    "    for i, row in results.iterrows():\n",
    "        if not isNaN(results[\"SMILES\"][i]):\n",
    "            if (\n",
    "                \"invalid_SMILES\" not in results[\"SMILES\"][i]\n",
    "                and \"invalid_chemistry\" not in results[\"SMILES\"][i]\n",
    "            ):\n",
    "                for j in cd:\n",
    "                    if not isNaN(j):\n",
    "                        CGms = [\n",
    "                            Chem.MolFromSmiles(results[\"SMILES\"][i]),\n",
    "                            Chem.MolFromSmiles(j),\n",
    "                        ]\n",
    "                        CGfps = [\n",
    "                            AllChem.GetMorganFingerprintAsBitVect(x, 2, nBits=1024)\n",
    "                            for x in CGms\n",
    "                        ]\n",
    "                        CGtn = DataStructs.FingerprintSimilarity(CGfps[0], CGfps[1])\n",
    "                        if (\n",
    "                            CGtn == 1\n",
    "                            and listname not in results[\"Annotation_Source\"][i]\n",
    "                        ):\n",
    "                            results[\"Annotation_Source\"][i] = (\n",
    "                                results[\"Annotation_Source\"][i] + \", \" + listname\n",
    "                            )\n",
    "\n",
    "    frame.to_csv(\n",
    "        input_dir + \"MetabolomicsResults/final_curationListVS\" + listname + \".csv\"\n",
    "    )\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650483f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f765fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(input_dir, resultcsv):\n",
    "   \n",
    "\n",
    "    \"\"\"classification function uses ClassyFire ChemONT\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML\n",
    "    files and their respective result directories are stored.\n",
    "\n",
    "    resultcsv: csv of df from combine_CuratedR or checkSMILES_validity\n",
    "\n",
    "    Returns:\n",
    "    dataframe: with classification\n",
    "    csv: \"MetabolomicsResults/final_curationList.csv\"\n",
    "\n",
    "    Usage:\n",
    "    checkSMILES_validity(input_dir = \"usr/project/\", frame)\n",
    "\n",
    "    \"\"\"\n",
    "    frame = pd.read_csv(resultcsv)\n",
    "    inchis = []\n",
    "    for i, row in frame.iterrows():\n",
    "        if not isNaN(frame[\"SMILES\"][i]):\n",
    "            try:\n",
    "                InChI = Chem.MolToInchi(Chem.MolFromSmiles(frame[\"SMILES\"][i]))\n",
    "                InChIKey = Chem.inchi.InchiToInchiKey(InChI)\n",
    "                inchis.append(\n",
    "                    {\n",
    "                        \"index\": i,\n",
    "                        \"smiles\": frame[\"SMILES\"][i],\n",
    "                        \"inchi\": InChI,\n",
    "                        \"inchikey\": InChIKey,\n",
    "                    }\n",
    "                )\n",
    "            except Exception:\n",
    "                pass\n",
    "    inchis = pd.DataFrame(inchis)\n",
    "    if len(inchis):\n",
    "        inchis = inchis.loc[-isNaN(inchis[\"inchikey\"])]\n",
    "        # Retrieve ClassyFire classifications\n",
    "\n",
    "        # This first step is done using inchikey and interrogation of the gnps classified structures\n",
    "        \"\"\"\n",
    "        gnps_proxy = True\n",
    "        url = \"http://classyfire.wishartlab.com\"\n",
    "        proxy_url = \"https://gnps-classyfire.ucsd.edu\"\n",
    "        chunk_size = 1000\n",
    "        sleep_interval = 12\n",
    "        \"\"\"\n",
    "\n",
    "        all_inchi_keys = list(inchis[\"inchikey\"].drop_duplicates())\n",
    "\n",
    "        resolved_ik_number_list = [0, 0]\n",
    "        # total_inchikey_number = len(all_inchi_keys)\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # start_time = time.time()\n",
    "\n",
    "            # print('%s inchikey to resolve' % total_inchikey_number )\n",
    "            get_classifications_cf_mod(all_inchi_keys, par_level=6)\n",
    "\n",
    "            cleanse(\"all_json.json\", \"all_json.json\")\n",
    "\n",
    "            with open(\"all_json.json\") as tweetfile:\n",
    "                jsondic = json.loads(tweetfile.read())\n",
    "\n",
    "            df = json_normalize(jsondic)\n",
    "            df = df.drop_duplicates(\"inchikey\")\n",
    "            resolved_ik_number = len(df.drop_duplicates(\"inchikey\").inchikey)\n",
    "            resolved_ik_number_list.append(resolved_ik_number)\n",
    "            # print('%s resolved inchikeys' % resolved_ik_number )\n",
    "            # print(\"done in --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "            if (\n",
    "                resolved_ik_number_list[-1] < resolved_ik_number_list[-2]\n",
    "                or resolved_ik_number_list[-1] == resolved_ik_number_list[-3]\n",
    "            ):\n",
    "                break\n",
    "            cleanse(\"all_json.json\", \"all_json_cleaned.json\")\n",
    "\n",
    "            with open(\"all_json_cleaned.json\") as tweetfile:\n",
    "                jsondic = json.loads(tweetfile.read())\n",
    "\n",
    "        flattened_classified_json = json_normalize(jsondic)\n",
    "        flattened_df = flattened_classified_json.drop_duplicates(\"inchikey\")\n",
    "        flattened_df[\"inchikey\"] = flattened_df[\"inchikey\"].str.replace(\n",
    "            r\"InChIKey=\", \"\"\n",
    "        )\n",
    "        df_merged = pd.merge(\n",
    "            inchis, flattened_df, left_on=\"inchikey\", right_on=\"inchikey\", how=\"left\"\n",
    "        )\n",
    "\n",
    "        for p, rowp in df_merged.iterrows():\n",
    "            for q, rowq in frame.iterrows():\n",
    "                if df_merged[\"smiles_x\"][p] is frame[\"SMILES\"][q]:\n",
    "                    frame.loc[q, \"subclass\"] = df_merged[\"subclass.name\"][p]\n",
    "                    frame.loc[q, \"class\"] = df_merged[\"class.name\"][p]\n",
    "                    frame.loc[q, \"superclass\"] = df_merged[\"superclass.name\"][p]\n",
    "                    frame.loc[q, \"ClassificationSource\"] = \"ClassyFire\"\n",
    "\n",
    "        frame.to_csv(input_dir + \"/final_candidates_classes.csv\")\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82f0a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# NP_Classifier classification\n",
    "def Np_pathways(input_dir, resultcsv):\n",
    "    df = pd.read_csv(resultcsv)\n",
    "    npresults = []\n",
    "    for i, row in df.iterrows():\n",
    "        if not isNaN(df[\"SMILES\"][i]):\n",
    "            try:\n",
    "                cvv = Chem.MolFromSmiles(df[\"SMILES\"][i])\n",
    "                cvv = Chem.MolToSmiles(cvv, isomericSmiles=False)\n",
    "                c = urllib.parse.quote_plus(cvv, safe=\" \")\n",
    "\n",
    "                url = \"https://npclassifier.ucsd.edu/classify?smiles=\" + c\n",
    "                names = str(df[\"id_X\"][i])\n",
    "                outx = str(\"myFile\" + names + \".txt\")\n",
    "                file = wget.download(url, out=outx)\n",
    "                a_dataframe = pd.read_csv(file, delimiter=\"]\")\n",
    "                xox = list(a_dataframe.columns.values)\n",
    "                splitting0 = xox[0].split(\":\")\n",
    "                xoc = re.sub('\\ |\\[|\\]|\"', \" \", splitting0[1]).strip()\n",
    "                splitting1 = xox[1].split(\":\")\n",
    "                xos = re.sub('\\ |\\[|\\]|\"', \" \", splitting1[1]).strip()\n",
    "                # except:\n",
    "                # splitting1 = xox[1].split(':')\n",
    "                # xos = re.sub('\\ |\\[|\\]|\\\"', '', splitting1[0])\n",
    "                splitting2 = xox[2].split(\":\")\n",
    "                xop = re.sub('\\ |\\[|\\]|\"', \" \", splitting2[1]).strip()\n",
    "                # df.loc[i, 'npclass'] = xoc\n",
    "                # df.loc[i, 'npsuper_class'] = xos\n",
    "                if not isNaN(df[\"class\"][i]) and df[\"class\"][i] in xoc:\n",
    "                    df.loc[i, \"np_pathway\"] = xop\n",
    "                os.remove(outx)\n",
    "                time.sleep(0.5)\n",
    "\n",
    "                npresults.append(\n",
    "                    {\n",
    "                        \"index\": i,\n",
    "                        # 'id': df['file_id'][i],\n",
    "                        \"mz\": df[\"premz\"][i],\n",
    "                        \"rt\": df[\"rtmed\"][i],\n",
    "                        \"SMILES\": df[\"SMILES\"][i],\n",
    "                        \"class\": xoc,\n",
    "                        \"subclass\": xos,\n",
    "                        \"pathway\": xop,\n",
    "                    }\n",
    "                )\n",
    "                if df[\"class\"][i] == xoc:\n",
    "                    df.loc[i, \"pathway\"]\n",
    "            except Exception:\n",
    "                pass\n",
    "    np_results = pd.DataFrame(npresults)\n",
    "    np_results.to_csv(input_dir + \"/NPClassifier_Results.csv\")\n",
    "    df.to_csv(input_dir + \"/final_results_with_Pathways.csv\")\n",
    "\n",
    "\n",
    "# Chemical Similarity MN\n",
    "def chemMN(input_dir, resultcsv):\n",
    "    # read csv\n",
    "    df = pd.read_csv(resultcsv)\n",
    "\n",
    "    # define empty variable\n",
    "    dbn = []\n",
    "\n",
    "    # check the result csv\n",
    "    for i, row in df.iterrows():\n",
    "        # to compare each element with each opther element\n",
    "        for j, row in df.iterrows():\n",
    "\n",
    "            # if its not same id\n",
    "            if df[\"SMILES\"][i] != df[\"SMILES\"][j]:\n",
    "\n",
    "                if not isNaN(df[\"SMILES\"][i]):\n",
    "                    if not isNaN(df[\"SMILES\"][j]):\n",
    "\n",
    "                        try:\n",
    "                            ms = [\n",
    "                                Chem.MolFromSmiles(df[\"SMILES\"][i]),\n",
    "                                Chem.MolFromSmiles(df[\"SMILES\"][j]),\n",
    "                            ]\n",
    "                            fps = [\n",
    "                                AllChem.GetMorganFingerprintAsBitVect(x, 2, nBits=2048)\n",
    "                                for x in ms\n",
    "                            ]\n",
    "                            tn = DataStructs.FingerprintSimilarity(fps[0], fps[1])\n",
    "                            dbn.append(\n",
    "                                {\n",
    "                                    \"Name_i\": df[\"IUPAC\"][i],\n",
    "                                    \"Name_j\": df[\"IUPAC\"][j],\n",
    "                                    \"i\": df[\"SMILES\"][i],\n",
    "                                    \"j\": df[\"SMILES\"][j],\n",
    "                                    \"Tanimoto\": tn,\n",
    "                                }\n",
    "                            )\n",
    "                        except Exception:\n",
    "                            pass\n",
    "    # save chemical similarities\n",
    "    db_edgenode = pd.DataFrame(dbn)\n",
    "\n",
    "    dfe = []\n",
    "    heavy_atoms = [\"C\", \"N\", \"P\", \"O\", \"S\"]\n",
    "    for i, row in db_edgenode.iterrows():\n",
    "        if db_edgenode[\"Tanimoto\"][i] >= 0.85:\n",
    "            # list of mol used to calaculate the MCSS\n",
    "            n = [\n",
    "                Chem.MolFromSmiles(db_edgenode[\"i\"][i]),\n",
    "                Chem.MolFromSmiles(db_edgenode[\"j\"][i]),\n",
    "            ]\n",
    "            res = rdFMCS.FindMCS(n)\n",
    "            sm_res = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "            # Check if the MCSS has one of the heavy atoms and whether they are\n",
    "            # more than 3\n",
    "            elem = [ele for ele in heavy_atoms if (ele in sm_res)]\n",
    "            if elem and len(sm_res) >= 3:\n",
    "                MCSS_SMILES = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "\n",
    "            dfe.append(\n",
    "                {\n",
    "                    \"Start\": db_edgenode[\"Name_i\"][i],\n",
    "                    \"End\": db_edgenode[\"Name_j\"][i],\n",
    "                    \"Tanimoto\": db_edgenode[\"Tanimoto\"][i],\n",
    "                    \"Start_SMILES\": db_edgenode[\"i\"][i],\n",
    "                    \"End_SMILES\": db_edgenode[\"j\"][i],\n",
    "                    # 'Start_Source':db_edgenode['Source_i'][i],\n",
    "                    # 'End_Source':db_edgenode['Source_j'][i],\n",
    "                    \"MCSS\": MCSS_SMILES,\n",
    "                }\n",
    "            )\n",
    "    if len(df_edge) > 0:\n",
    "        \n",
    "        df_edge = pd.DataFrame(dfe)\n",
    "        df_edge[\"Start\"] = df_edge[\"Start\"].astype(str)\n",
    "        df_edge[\"End\"] = df_edge[\"End\"].astype(str)\n",
    "        df_edge[\"sorted_row\"] = [sorted([a, b]) for a, b in zip(df_edge.Start, df_edge.End)]\n",
    "        df_edge[\"sorted_row\"] = df_edge[\"sorted_row\"].astype(str)\n",
    "        df_edge.drop_duplicates(subset=[\"sorted_row\"], inplace=True)\n",
    "\n",
    "        nodes = []\n",
    "        for i, row in df.iterrows():\n",
    "            n = df[\"IUPAC\"][i]\n",
    "            nodes.append({\"nodes\": n})\n",
    "\n",
    "        node = pd.DataFrame(nodes)\n",
    "\n",
    "        df_edge.to_csv(input_dir + \"/ChemMNedges.tsv\", sep=\"\\t\")\n",
    "        node.to_csv(input_dir + \"/ChemMNnodes.csv\", index=False)\n",
    "\n",
    "        newdf = df_edge\n",
    "        newdf[\"StartAtt\"] = np.nan\n",
    "        newdf[\"EndAtt\"] = np.nan\n",
    "        for i, row in newdf.iterrows():\n",
    "            for j, row in df.iterrows():\n",
    "                if newdf[\"Start\"][i] == df[\"IUPAC\"][j]:\n",
    "                    newdf.loc[i, \"StartAtt\"] = df[\"class\"][j]\n",
    "                if newdf[\"End\"][i] == df[\"IUPAC\"][j]:\n",
    "                    newdf.loc[i, \"EndAtt\"] = df[\"class\"][j]\n",
    "        newdf.to_csv(input_dir + \"/ChemMNcys.tsv\", sep=\"\\t\")\n",
    "\n",
    "        return newdf\n",
    "\n",
    "\n",
    "def gnpsMNvsgnpsMAW(input_dir):\n",
    "    \n",
    "    \"\"\"gnpsMNvsgnpsMAW checks with tanimoto similarity score, whether\n",
    "    results from MAW GNPS and GNPS MN Masst results give same candidate\n",
    "\n",
    "    Parameters:\n",
    "    input_dir = input directory where you have stored the cytoscape file\n",
    "    from GNPS MN results and have exported edge and node tables from cytoscape\n",
    "    These two csv egde and node files must have \"edge\" and \"node\" in their name\n",
    "\n",
    "    Returns:\n",
    "    GNPS results with cluster index named\n",
    "    GNPS MN results with a confirmation column if MAW detected same candidate,\n",
    "    file named:\n",
    "\n",
    "    Usage:\n",
    "    gnpsMNvsgnpsMAW(input_dir)\n",
    "\n",
    "    \"\"\"\n",
    "    # extract files with edges from MN results\n",
    "    GMNfile_edge = [f for f in os.listdir(input_dir) if \"edge\" in f]\n",
    "    # extract files with nodes from MN results\n",
    "    GMNfile_node = [f for f in os.listdir(input_dir) if \"node\" in f]\n",
    "    # read the files\n",
    "    GMNdf_node = pd.read_csv(GMNfile_node[0])\n",
    "    GMNdf_edge = pd.read_csv(GMNfile_edge[0])\n",
    "\n",
    "    # extract only important columns from both csv files\n",
    "    GMNdf_node = GMNdf_node[\n",
    "        [\n",
    "            \"precursor mass\",\n",
    "            \"RTMean\",\n",
    "            \"UniqueFileSources\",\n",
    "            \"charge\",\n",
    "            \"cluster index\",\n",
    "            \"componentindex\",\n",
    "            \"Compound_Name\",\n",
    "            \"Smiles\",\n",
    "            \"SpectrumID\",\n",
    "        ]\n",
    "    ]\n",
    "    GMNdf_edge = GMNdf_edge[\n",
    "        [\"cosine_score\", \"EdgeAnnotation\", \"node1\", \"node2\", \"mass_difference\"]\n",
    "    ]\n",
    "\n",
    "    # rename node1 to cluster index to merge nodes and edges results from MN\n",
    "    GMNdf_edge = GMNdf_edge.rename(columns={\"node1\": \"cluster index\"})\n",
    "    GMNdf = pd.merge(GMNdf_node, GMNdf_edge, on=\"cluster index\")\n",
    "\n",
    "    # Read results obtained from scoring_spec, named input_dir/MetabolomicsResults/scoredSpecDB.csv\n",
    "    SDB = pd.read_csv(input_dir + \"/MetabolomicsResults/scoredSpecDB.csv\")\n",
    "    # only keep GNPS resulst and remove other columns\n",
    "    only_GNPS = SDB[SDB[\"annotation\"].str.contains(\"GNPS\")]\n",
    "    only_GNPS = only_GNPS[\n",
    "        [\n",
    "            \"id_X\",\n",
    "            \"premz_x\",\n",
    "            \"rtmean_x\",\n",
    "            \"GNPSmax_similarity\",\n",
    "            \"GNPSSMILES\",\n",
    "            #\"GNPSspectrumID\",\n",
    "            \"GNPScompound_name\",\n",
    "            \"GNPSmirrorSpec\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # from GNPS MAW results and GNPS MN results, calculate how many MAW results are same as MN:\n",
    "    for i, row in only_GNPS.iterrows():\n",
    "        for j, row in GMNdf.iterrows():\n",
    "            if not isNaN(only_GNPS[\"GNPSSMILES\"][i]) and not isNaN(GMNdf[\"Smiles\"][j]):\n",
    "                SKms = [\n",
    "                    Chem.MolFromSmiles(only_GNPS[\"GNPSSMILES\"][i]),\n",
    "                    Chem.MolFromSmiles(GMNdf[\"Smiles\"][j]),\n",
    "                ]\n",
    "                SKfps = [\n",
    "                    AllChem.GetMorganFingerprintAsBitVect(x, 2, nBits=2048)\n",
    "                    for x in SKms\n",
    "                ]\n",
    "                SKtn = DataStructs.FingerprintSimilarity(SKfps[0], SKfps[1])\n",
    "                if SKtn == 1.0:\n",
    "                    GMNdf.loc[j, \"gnps_maw\"] = \"confirmed\"\n",
    "                    only_GNPS.loc[i, \"index_MN_nodes\"] = j\n",
    "                elif SKtn < 1.0 and SKtn < 0.75:\n",
    "                    GMNdf.loc[j, \"gnps_maw\"] = \"similar\"\n",
    "                    only_GNPS.loc[i, \"index_MN_nodes\"] = j\n",
    "    only_GNPS.to_csv(input_dir + \"/only_GNPS.csv\")\n",
    "    GMNdf.to_csv(input_dir + \"/GMNdf.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mawRpy)",
   "language": "python",
   "name": "mawrpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
